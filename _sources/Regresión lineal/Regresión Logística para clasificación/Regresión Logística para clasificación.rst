RegresiÃ³n LogÃ­stica para clasificaciÃ³n
--------------------------------------

La regresiÃ³n logÃ­stica se utiliza muchas veces como herramienta de
clasificaciÃ³n; sin embargo, cabe seÃ±alar que la capacidad de clasificar
o discriminar entre los dos niveles de la variable respuesta se debe mÃ¡s
al grado de separaciÃ³n entre los niveles y al tamaÃ±o de los coeficientes
de regresiÃ³n que al propio modelo logÃ­stico.

Hay dos herramientas de clasificaciÃ³n que se utilizan con la regresiÃ³n
logÃ­stica:

1. Curva ROC (Receiver Operating Characteristics).

2. Matriz de confusiÃ³n.

Cada una de estas herramientas se basan en un punto de corte. El punto
de corte es la probabilidad Ã³ptima con que se separan las observaciones
en 1 y 0. AsÃ­ se analizan las predicciones acertadas y las no acertadas.

Los modelos de regresiÃ³n logÃ­stica pueden hacer una buena clasificaciÃ³n
y no siempre son modelos bien ajustados. Si el interÃ©s es estrictamente
la clasificaciÃ³n, no es tan importante el ajuste del modelo. Asimismo,
un modelo logÃ­stico bien ajustado puede no diferenciar claramente los
dos niveles de la variable respuesta.

**Curva ROC:**

La curva ROC se usa para examinar el equilibrio entre la detecciÃ³n de
verdaderos positivos y evitar los falsos positivos. La curva se define
con la proporciÃ³n de verdaderos positivos en el eje vertical y la
proporciÃ³n de faltos positivos en el eje horizontal.

**Punto de corte:** para clasificar las predicciones en :math:`1` y
:math:`0` se debe definir un punto de corte, este es la probabilidad que
debemos definir y las predicciones iguales o mayores que esta
probabilidad las etiquetaremos como :math:`1` y las predicciones con
probabilidades menores como :math:`0`. Por ejemplo, si definimos el
punto de corte como 0,70, las observaciones con probabilidades mayores o
iguales que 0,50 serÃ¡ la categorÃ­a :math:`1` y las demÃ¡s observaciones
serÃ¡n :math:`0`.

DespuÃ©s de definir el punto de corte y de realizar la clasificaciÃ³n, la
matriz de confusiÃ³n nos mostrarÃ¡ las observaciones que fueron bien
clasificadas y las que se les asignÃ³ una clasificaciÃ³n incorrecta.

Los puntos que comprende la curva ROC indican la tasa de verdaderos
positivos en diferentes umbrales de falsos positivos. Para crear la
curva, las predicciones de la clasificaciÃ³n se ordenan segÃºn la
probabilidad estimada del modelo para la clase positiva, con los valores
grandes primero. Comenzando en el origen, el impacto de cada predicciÃ³n
en la tasa de verdaderos positivos y la tasa de falsos positivos darÃ¡
como resultado una curva que se traza verticalmente (para una predicciÃ³n
correcta) u horizontal (para una predicciÃ³n incorrecta).

La lÃ­nea diagonal desde la esquina inferior izquierda hasta la esquina
superior derecha del diagrama representa un clasificador sin valor
predictivo. Este tipo de clasificador detecta verdaderos positivos y
falsos positivos exactamente a la misma velocidad, lo que implica que el
clasificador no puede discriminar entre las dos categorÃ­as (:math:`1` y
:math:`0`). Esta es la lÃ­nea de base por la cual se pueden juzgar otros
clasificadores. Las curvas ROC que caen cerca de esta lÃ­nea indican
modelos que no son muy Ãºtiles.

De manera similar, el clasificador perfecto tiene una curva que pasa por
el punto con una tasa de 100 por ciento de verdaderos positivos y 0 por
ciento de tasa de falsos positivos. Es capaz de identificar
correctamente todos los verdaderos positivos antes de clasificar
incorrectamente cualquier resultado negativo.

.. figure:: CurvaROC.JPG
   :alt: CurvaROC

   CurvaROC

**Sensitivity:** es la tasa de los verdaderos positivos.

**Specificity:** es la tasa de los falsos positivos (1 - Specificity).

Cuanto mÃ¡s cerca estÃ© la curva del clasificador perfecto, mejor serÃ¡
para identificar valores positivos. Esto se puede medir utilizando una
estadÃ­stica conocida como el Ã¡rea bajo la curva ROC - AUC (area under
the ROC curve). El AUC mide el Ã¡rea total bajo la curva ROC. AUC varÃ­a
de 0,5 (para un clasificador sin valor predictivo) a 1,0 (para un
clasificador perfecto). Una convenciÃ³n para interpretar las puntuaciones
AUC utiliza un sistema similar a las calificaciones acadÃ©micas con
letras:

-  0.9 â€“ 1.0 = Excelente

-  0,8 â€“ 0,9 = Bueno

-  0.7 â€“ 0.8 = Aceptable

-  0.6 â€“ 0.7 = Pobre

-  0,5 â€“ 0,6 = Sin discriminaciÃ³n

El Ã¡rea bajo la curva ROC (AUC) es una mÃ©trica que evalÃºa quÃ© tan bien
un modelo de regresiÃ³n logÃ­stica clasifica los resultados positivos y
negativos en todos los lÃ­mites posibles.

Resulta que el AUC es la probabilidad de que si tomara un par de
observaciones al azar, una con :math:`Y=1` y otra con :math:`Y=0`, la
observaciÃ³n con :math:`Y=1` tiene una probabilidad predicha mÃ¡s alta que
la otra. El AUC, por lo tanto, da la probabilidad de que el modelo
clasifique correctamente tales pares de observaciones.

TambiÃ©n vale la pena seÃ±alar que dos curvas ROC pueden tener una forma
muy diferente y, sin embargo, tener un AUC idÃ©ntico. Por esta razÃ³n, AUC
puede ser extremadamente engaÃ±oso. La mejor prÃ¡ctica es usar AUC en
combinaciÃ³n con un examen cualitativo de la curva ROC. De manera
similar, el clasificador perfecto tiene una curva que pasa por el punto
con una tasa de 100 por ciento de verdaderos positivos y 0 por ciento de
tasa de falsos positivos. Es capaz de identificar correctamente todos
los verdaderos positivos antes de clasificar incorrectamente cualquier
resultado negativo.

**Matriz de confusiÃ³n:**

Una matriz de confusiÃ³n es una tabla que clasifica las predicciones
segÃºn sÃ­ coinciden con el valor real de los datos. Una de las
dimensiones de la tabla indica las posibles categorÃ­as de valores
predichos mientras que la otra dimensiÃ³n indica lo mismo para los
valores reales. Aunque solo vemos matrices de confusiÃ³n de 2 x 2, se
puede crear una matriz para un modelo que predice cualquier nÃºmero de
categorÃ­as.

Cuando el valor predicho es el mismo que el valor real, esta es una
clasificaciÃ³n correcta. Las predicciones correctas caen en la diagonal
de la matriz de confusiÃ³n. Las celdas de la matriz fuera de la diagonal
indican los casos en los que el valor predicho difiere del valor real.
Estas son predicciones incorrectas. Las medidas de rendimiento para los
modelos de clasificaciÃ³n se basan en los recuentos de predicciones que
entran y salen de la diagonal en estas tablas.

.. figure:: MatrizConfusion.JPG
   :alt: MatrizConfusion

   MatrizConfusion

La matriz de confusiÃ³n tabula las predicciones en cuatro categorÃ­as:

1. **Verdadero Positivo - TP (True Positive):** clasifica correctamente
   la categorÃ­a :math:`1` de la variable de interÃ©s.

2. **Verdadero Negativo - TN (True Negative):** clasifica correctamente
   la categorÃ­a :math:`0` de la variable de interÃ©s.

3. **Falso positivo - FP (False Positive):** clasifica incorrectamente
   la categorÃ­a :math:`1`.

4. **Falso negativo - FN (False Negative):** clasificia incorrectamente
   la categorÃ­a :math:`0`.

.. figure:: MatrizConfusion2.JPG
   :alt: MatrizConfusion2

   MatrizConfusion2

MÃ©tricas:
~~~~~~~~~

Con la matriz de confusiÃ³n podemos calcular las siguientes mÃ©tricas.

**Accuracy:**

.. math::  accuracy = \frac{TP+TN}{TP+TN+FP+FN}  

El **accuracy** es la proporciÃ³n que representa el nÃºmero de verdaderos
positivos y verdaderos negativos dividido por el nÃºmero total de
predicciones.

.. figure:: Accuracy.JPG
   :alt: Accuracy

   Accuracy

**Error Rate:**

Lo contrario al accuracy es el **error.**

.. math::  ErrorRate = \frac{FP+FN}{TP+TN+FP+FN} = 1 - accuracy 

.. figure:: ErrorRate.JPG
   :alt: ErrorRate

   ErrorRate

**Sensitivity:**

La sensibilidad de un modelo **(Sensitivity)**, tambiÃ©n denominada tasa
de verdaderos positivos (TP), mide la proporciÃ³n de observaciones
positivas que se clasificaron correctamente. Por lo tanto, como se
muestra en la siguiente fÃ³rmula, se calcula como el nÃºmero de verdaderos
positivos dividido por el nÃºmero total de positivos en los datos: los
clasificados correctamente (los verdaderos positivos), asÃ­ como los
clasificados incorrectamente (los falsos negativos).

.. math::  sensitivity = \frac{TP}{TP+FN}  

.. figure:: Sensitivity.JPG
   :alt: Sensitivity

   Sensitivity

**Specificity:**

La especificidad de un modelo **(Specificity)**, tambiÃ©n llamada tasa de
negativos verdaderos (TN), mide la proporciÃ³n de observaciones negativas
que se clasificaron correctamente. Al igual que con la sensibilidad,
esto se calcula como el nÃºmero de negativos verdaderos dividido por el
nÃºmero total de negativos: los negativos verdaderos mÃ¡s los falsos
positivos.

.. math::  specificity = \frac{TN}{TN+FP}  

.. figure:: Specificity.JPG
   :alt: Specificity

   Specificity

Otras dos medidas de desempeÃ±o relacionadas con la sensitivity y la
specificity son **precision** y **recall**.

**Precision:**

**Precision** es la proporciÃ³n de observaciones positivas que son
positivo verdadero (TP), en otras palabras, cuando el modelo de
clasificaciÃ³n predice la categorÃ­a de :math:`1`, esta mÃ©trica indica la
frecuencia de estar en lo cierto. Un modelo preciso solo predecirÃ¡ la
categorÃ­a positiva :math:`(1)` en casos muy probables de ser positivos.

.. math::  precision = \frac{TP}{TP+FP}  

.. figure:: Precision.JPG
   :alt: Precision

   Precision

**Recall:**

**Recall** es una medida de quÃ© tan completos son los resultados. Es el
nÃºmero de positivos verdaderos (TP) sobre el total de positivos. Esto es
lo mismo que la sensibilidad, pero se podrÃ­a interpretar diferente. Un
modelo con alto recall captura gran parte de las observaciones
positivas, lo que significa que tienen una gran amplitud.

.. math::  recall = \frac{TP}{TP+FN}  

.. figure:: Precision-recall.JPG
   :alt: Precision-recall

   Precision-recall

**F-measure:**

Una medida del rendimiento del modelo que combina ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› y ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ en
un solo nÃºmero se conoce como **medida F** (F-measure), a veces tambiÃ©n
llamada puntuaciÃ³n F1 o puntuaciÃ³n F. La medida F combina ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› y
ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ utilizando la media armÃ³nica. Se utiliza la media armÃ³nica en
lugar de la media aritmÃ©tica, ya que tanto ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› como ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ se
expresan como proporciones entre cero y uno.

.. math::  F-measure = \frac{2 \times precision \times recall}{recall + precision} = \frac{2 \times TP}{2 \times TP + FP + FN}  
