Regresi√≥n Log√≠stica para clasificaci√≥n
--------------------------------------

La regresi√≥n log√≠stica se utiliza muchas veces como herramienta de
clasificaci√≥n; sin embargo, cabe se√±alar que la capacidad de clasificar
o discriminar entre los dos niveles de la variable respuesta se debe m√°s
al grado de separaci√≥n entre los niveles y al tama√±o de los coeficientes
de regresi√≥n que al propio modelo log√≠stico.

Hay dos herramientas de clasificaci√≥n que se utilizan con la regresi√≥n
log√≠stica:

1. Curva ROC (Receiver Operating Characteristics).

2. Matriz de confusi√≥n.

Cada una de estas herramientas se basan en un punto de corte. El punto
de corte es la probabilidad √≥ptima con que se separan las observaciones
en 1 y 0. As√≠ se analizan las predicciones acertadas y las no acertadas.

Los modelos de regresi√≥n log√≠stica pueden hacer una buena clasificaci√≥n
y no siempre son modelos bien ajustados. Si el inter√©s es estrictamente
la clasificaci√≥n, no es tan importante el ajuste del modelo. Asimismo,
un modelo log√≠stico bien ajustado puede no diferenciar claramente los
dos niveles de la variable respuesta.

**Curva ROC:**

La curva ROC se usa para examinar el equilibrio entre la detecci√≥n de
verdaderos positivos y evitar los falsos positivos. La curva se define
con la proporci√≥n de verdaderos positivos en el eje vertical y la
proporci√≥n de faltos positivos en el eje horizontal.

**Punto de corte:** para clasificar las predicciones en :math:`1` y
:math:`0` se debe definir un punto de corte, este es la probabilidad que
debemos definir y las predicciones iguales o mayores que esta
probabilidad las etiquetaremos como :math:`1` y las predicciones con
probabilidades menores como :math:`0`. Por ejemplo, si definimos el
punto de corte como 0,70, las observaciones con probabilidades mayores o
iguales que 0,50 ser√° la categor√≠a :math:`1` y las dem√°s observaciones
ser√°n :math:`0`.

Despu√©s de definir el punto de corte y de realizar la clasificaci√≥n, la
matriz de confusi√≥n nos mostrar√° las observaciones que fueron bien
clasificadas y las que se les asign√≥ una clasificaci√≥n incorrecta.

Los puntos que comprende la curva ROC indican la tasa de verdaderos
positivos en diferentes umbrales de falsos positivos. Para crear la
curva, las predicciones de la clasificaci√≥n se ordenan seg√∫n la
probabilidad estimada del modelo para la clase positiva, con los valores
grandes primero. Comenzando en el origen, el impacto de cada predicci√≥n
en la tasa de verdaderos positivos y la tasa de falsos positivos dar√°
como resultado una curva que se traza verticalmente (para una predicci√≥n
correcta) u horizontal (para una predicci√≥n incorrecta).

La l√≠nea diagonal desde la esquina inferior izquierda hasta la esquina
superior derecha del diagrama representa un clasificador sin valor
predictivo. Este tipo de clasificador detecta verdaderos positivos y
falsos positivos exactamente a la misma velocidad, lo que implica que el
clasificador no puede discriminar entre las dos categor√≠as (:math:`1` y
:math:`0`). Esta es la l√≠nea de base por la cual se pueden juzgar otros
clasificadores. Las curvas ROC que caen cerca de esta l√≠nea indican
modelos que no son muy √∫tiles.

De manera similar, el clasificador perfecto tiene una curva que pasa por
el punto con una tasa de 100 por ciento de verdaderos positivos y 0 por
ciento de tasa de falsos positivos. Es capaz de identificar
correctamente todos los verdaderos positivos antes de clasificar
incorrectamente cualquier resultado negativo.

.. figure:: CurvaROC.JPG
   :alt: CurvaROC

   CurvaROC

**Sensitivity:** es la tasa de los verdaderos positivos.

**Specificity:** es la tasa de los falsos positivos (1 - Specificity).

Cuanto m√°s cerca est√© la curva del clasificador perfecto, mejor ser√°
para identificar valores positivos. Esto se puede medir utilizando una
estad√≠stica conocida como el √°rea bajo la curva ROC - AUC (area under
the ROC curve). El AUC mide el √°rea total bajo la curva ROC. AUC var√≠a
de 0,5 (para un clasificador sin valor predictivo) a 1,0 (para un
clasificador perfecto). Una convenci√≥n para interpretar las puntuaciones
AUC utiliza un sistema similar a las calificaciones acad√©micas con
letras:

-  0.9 ‚Äì 1.0 = Excelente

-  0,8 ‚Äì 0,9 = Bueno

-  0.7 ‚Äì 0.8 = Aceptable

-  0.6 ‚Äì 0.7 = Pobre

-  0,5 ‚Äì 0,6 = Sin discriminaci√≥n

El √°rea bajo la curva ROC (AUC) es una m√©trica que eval√∫a qu√© tan bien
un modelo de regresi√≥n log√≠stica clasifica los resultados positivos y
negativos en todos los l√≠mites posibles.

Resulta que el AUC es la probabilidad de que si tomara un par de
observaciones al azar, una con :math:`Y=1` y otra con :math:`Y=0`, la
observaci√≥n con :math:`Y=1` tiene una probabilidad predicha m√°s alta que
la otra. El AUC, por lo tanto, da la probabilidad de que el modelo
clasifique correctamente tales pares de observaciones.

Tambi√©n vale la pena se√±alar que dos curvas ROC pueden tener una forma
muy diferente y, sin embargo, tener un AUC id√©ntico. Por esta raz√≥n, AUC
puede ser extremadamente enga√±oso. La mejor pr√°ctica es usar AUC en
combinaci√≥n con un examen cualitativo de la curva ROC. De manera
similar, el clasificador perfecto tiene una curva que pasa por el punto
con una tasa de 100 por ciento de verdaderos positivos y 0 por ciento de
tasa de falsos positivos. Es capaz de identificar correctamente todos
los verdaderos positivos antes de clasificar incorrectamente cualquier
resultado negativo.

**Matriz de confusi√≥n:**

Una matriz de confusi√≥n es una tabla que clasifica las predicciones
seg√∫n s√≠ coinciden con el valor real de los datos. Una de las
dimensiones de la tabla indica las posibles categor√≠as de valores
predichos mientras que la otra dimensi√≥n indica lo mismo para los
valores reales. Aunque solo vemos matrices de confusi√≥n de 2 x 2, se
puede crear una matriz para un modelo que predice cualquier n√∫mero de
categor√≠as.

Cuando el valor predicho es el mismo que el valor real, esta es una
clasificaci√≥n correcta. Las predicciones correctas caen en la diagonal
de la matriz de confusi√≥n. Las celdas de la matriz fuera de la diagonal
indican los casos en los que el valor predicho difiere del valor real.
Estas son predicciones incorrectas. Las medidas de rendimiento para los
modelos de clasificaci√≥n se basan en los recuentos de predicciones que
entran y salen de la diagonal en estas tablas.

.. figure:: MatrizConfusion.JPG
   :alt: MatrizConfusion

   MatrizConfusion

La matriz de confusi√≥n tabula las predicciones en cuatro categor√≠as:

1. **Verdadero Positivo - TP (True Positive):** clasifica correctamente
   la categor√≠a :math:`1` de la variable de inter√©s.

2. **Verdadero Negativo - TN (True Negative):** clasifica correctamente
   la categor√≠a :math:`0` de la variable de inter√©s.

3. **Falso positivo - FP (False Positive):** clasifica incorrectamente
   la categor√≠a :math:`1`.

4. **Falso negativo - FN (False Negative):** clasificia incorrectamente
   la categor√≠a :math:`0`.

.. figure:: MatrizConfusion2.JPG
   :alt: MatrizConfusion2

   MatrizConfusion2

M√©tricas:
~~~~~~~~~

Con la matriz de confusi√≥n podemos calcular las siguientes m√©tricas.

**Accuracy:**

.. math::  accuracy = \frac{TP+TN}{TP+TN+FP+FN}  

El **accuracy** es la proporci√≥n que representa el n√∫mero de verdaderos
positivos y verdaderos negativos dividido por el n√∫mero total de
predicciones.

.. figure:: Accuracy.JPG
   :alt: Accuracy

   Accuracy

**Error Rate:**

Lo contrario al accuracy es el **error.**

.. math::  ErrorRate = \frac{FP+FN}{TP+TN+FP+FN} = 1 - accuracy 

.. figure:: ErrorRate.JPG
   :alt: ErrorRate

   ErrorRate

**Sensitivity:**

La sensibilidad de un modelo **(Sensitivity)**, tambi√©n denominada tasa
de verdaderos positivos (TP), mide la proporci√≥n de observaciones
positivas que se clasificaron correctamente. Por lo tanto, como se
muestra en la siguiente f√≥rmula, se calcula como el n√∫mero de verdaderos
positivos dividido por el n√∫mero total de positivos en los datos: los
clasificados correctamente (los verdaderos positivos), as√≠ como los
clasificados incorrectamente (los falsos negativos).

.. math::  sensitivity = \frac{TP}{TP+FN}  

.. figure:: Sensitivity.JPG
   :alt: Sensitivity

   Sensitivity

**Specificity:**

La especificidad de un modelo **(Specificity)**, tambi√©n llamada tasa de
negativos verdaderos (TN), mide la proporci√≥n de observaciones negativas
que se clasificaron correctamente. Al igual que con la sensibilidad,
esto se calcula como el n√∫mero de negativos verdaderos dividido por el
n√∫mero total de negativos: los negativos verdaderos m√°s los falsos
positivos.

.. math::  specificity = \frac{TN}{TN+FP}  

.. figure:: Specificity.JPG
   :alt: Specificity

   Specificity

Otras dos medidas de desempe√±o relacionadas con la sensitivity y la
specificity son **precision** y **recall**.

**Precision:**

**Precision** es la proporci√≥n de observaciones positivas que son
positivo verdadero (TP), en otras palabras, cuando el modelo de
clasificaci√≥n predice la categor√≠a de :math:`1`, esta m√©trica indica la
frecuencia de estar en lo cierto. Un modelo preciso solo predecir√° la
categor√≠a positiva :math:`(1)` en casos muy probables de ser positivos.

.. math::  precision = \frac{TP}{TP+FP}  

.. figure:: Precision.JPG
   :alt: Precision

   Precision

**Recall:**

**Recall** es una medida de qu√© tan completos son los resultados. Es el
n√∫mero de positivos verdaderos (TP) sobre el total de positivos. Esto es
lo mismo que la sensibilidad, pero se podr√≠a interpretar diferente. Un
modelo con alto recall captura gran parte de las observaciones
positivas, lo que significa que tienen una gran amplitud.

.. math::  recall = \frac{TP}{TP+FN}  

.. figure:: Precision-recall.JPG
   :alt: Precision-recall

   Precision-recall

**F-measure:**

Una medida del rendimiento del modelo que combina ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ y ùëüùëíùëêùëéùëôùëô en
un solo n√∫mero se conoce como **medida F** (F-measure), a veces tambi√©n
llamada puntuaci√≥n F1 o puntuaci√≥n F. La medida F combina ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ y
ùëüùëíùëêùëéùëôùëô utilizando la media arm√≥nica. Se utiliza la media arm√≥nica en
lugar de la media aritm√©tica, ya que tanto ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ como ùëüùëíùëêùëéùëôùëô se
expresan como proporciones entre cero y uno.

.. math::  F-measure = \frac{2 \times precision \times recall}{recall + precision} = \frac{2 \times TP}{2 \times TP + FP + FN}  
