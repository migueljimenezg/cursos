AR
--

**Modelos Autorregresivos AR(p)**

Un modelo **autorregresivo (AR)** es un tipo de modelo de series de
tiempo donde el valor actual de la serie depende de sus propios valores
pasados m√°s un t√©rmino de error aleatorio. Se llaman *autorregresivos*
porque la serie ‚Äúse explica a s√≠ misma‚Äù a partir de sus rezagos.

En t√©rminos simples:

.. math::


   y_t = \alpha + \beta_t + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t

-  :math:`y_t`: valor actual de la serie

-  :math:`\alpha`: constante o media ajustada :math:`\mu`

-  :math:`\beta_t` es la **tendencia lineal en el tiempo**

-  :math:`\phi_1, \phi_2, \dots, \phi_p`: coeficientes que indican
   cu√°nto pesa cada valor pasado

-  :math:`p`: n√∫mero de rezagos que se usan (el ‚Äúorden‚Äù del modelo)

-  :math:`\varepsilon_t`: error aleatorio o ‚Äúruido blanco‚Äù (media cero,
   varianza constante, no correlacionado)

**AR(1): el caso m√°s simple**

En un **AR(1)** el valor actual depende **solo del √∫ltimo valor
pasado**:

.. math::


   y_t = \alpha + \phi_1 y_{t-1} + \varepsilon_t

-  Si :math:`\phi_1` es positivo y cercano a 1, la serie muestra mucha
   persistencia (los valores grandes suelen seguir a valores grandes).

-  Si :math:`\phi_1` es negativo, la serie tiende a oscilar: un valor
   alto es seguido por uno bajo, y viceversa.

-  Si :math:`\phi_1 = 0`, el modelo se reduce a una serie de ruido
   blanco.

Este modelo es an√°logo a una **regresi√≥n lineal simple**, donde la
variable dependiente es el valor actual y la variable explicativa es el
valor inmediatamente anterior.

**¬øPara qu√© tipos de series de tiempo se usa?**

Estacionarias en media y varianza (o que pueden hacerse estacionarias
con transformaciones/ diferencias).

Sin tendencia ni estacionalidad fuerte (si existen, se suelen remover
antes: diferenciar para tendencia; desestacionalizar o pasar a SARIMA).

Con memoria corta: el pasado cercano explica el presente y el efecto de
choques se amortigua con el tiempo.

**Propiedades b√°sicas:**

-  La media de la serie es constante si :math:`|\phi_1| < 1`.

-  La varianza es estable y finita bajo esa misma condici√≥n.

-  El efecto de un choque :math:`\varepsilon_t` se diluye en el tiempo
   de manera exponencial.

**AR(p): el caso general**

Un **AR(p)** ampl√≠a la idea al permitir que el valor actual dependa de
varios rezagos:

.. math::


   y_t = \alpha + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t

Es como una **regresi√≥n m√∫ltiple**, donde las variables explicativas son
los √∫ltimos :math:`p` valores de la serie. Cada coeficiente
:math:`\phi_i` indica cu√°nto influye el valor de la serie en el rezago
:math:`i`.

**C√≥mo se estiman los par√°metros**

El modelo se ajusta como una regresi√≥n:

.. math::


   \hat{y}_t = \hat{\alpha} + \hat{\phi}_1 y_{t-1} + \hat{\phi}_2 y_{t-2} + \cdots + \hat{\phi}_p y_{t-p}

Los residuos son:

.. math::


   \hat{\varepsilon}_t = y_t - \hat{y}_t

Y se espera que se comporten como ruido blanco.

Los coeficientes se pueden estimar mediante **m√≠nimos cuadrados** o
**m√°xima verosimilitud.**

**Analog√≠a sencilla**

Imagina que el **clima de hoy** depende de c√≥mo fue el clima de los
√∫ltimos d√≠as:

-  Si solo depende del **d√≠a anterior**, tenemos un AR(1).

-  Si adem√°s influye el clima de los **√∫ltimos tres d√≠as**, estamos en
   un AR(3).

El modelo AR ‚Äúaprende‚Äù cu√°nto peso darle a cada d√≠a pasado para hacer
una predicci√≥n del presente o del futuro.

**Condici√≥n de estabilidad y el valor de œÜ**

Para que un modelo AR(p) sea **estacionario y estable**, los par√°metros
deben cumplir ciertas condiciones.

En el caso de un **AR(1)**:

.. math::


   y_t = \alpha + \phi_1 y_{t-1} + \varepsilon_t

La condici√≥n es:

.. math::


   |\phi_1| < 1

**¬øPor qu√©?**

-  Si :math:`|\phi_1| < 1`, los efectos de un choque
   :math:`\varepsilon_t` se van diluyendo en el tiempo.

   Ejemplo: si :math:`\phi_1 = 0.7`, un shock de 10 se convierte en 7 el
   siguiente periodo, luego 4.9, luego 3.43, y as√≠ sucesivamente hasta
   desaparecer.

-  Si :math:`|\phi_1| = 1`, el efecto **no se disipa**: tenemos una ra√≠z
   unitaria. Esto corresponde a un **paseo aleatorio**, que no es
   estacionario porque la varianza crece sin l√≠mite.

-  Si :math:`|\phi_1| > 1`, los efectos de un shock se **amplifican**
   con el tiempo y la serie explota, volvi√©ndose inestable.

**Generalizaci√≥n al AR(p)**

En un modelo **AR(p)**:

.. math::


   y_t = \alpha + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t

la condici√≥n de estabilidad se expresa en t√©rminos del **polinomio
caracter√≠stico**:

.. math::


   1 - \phi_1 L - \phi_2 L^2 - \cdots - \phi_p L^p = 0

donde :math:`L` es el operador rezago.

-  Para que el modelo sea estable, **todas las ra√≠ces de este polinomio
   deben estar fuera del c√≠rculo unitario**, es decir, deben tener un
   m√≥dulo mayor que 1.

-  Dicho de otra forma: ning√∫n valor de :math:`L` dentro del c√≠rculo
   unitario (radio 1 en el plano complejo) debe anular la ecuaci√≥n.

**Intuici√≥n del c√≠rculo unitario**

El c√≠rculo unitario es una forma matem√°tica de decir:

-  **Dentro del c√≠rculo** (:math:`|\phi| < 1`): el proceso es estable,
   los choques se disipan.

-  **En el borde** (:math:`|\phi| = 1`): el proceso tiene ra√≠z unitaria,
   no es estacionario (ejemplo t√≠pico: paseo aleatorio).

-  **Fuera del c√≠rculo** (:math:`|\phi| > 1`): el proceso explota, los
   choques se amplifican con el tiempo.

El operador rezago (lag operator)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

El operador rezago se denota por :math:`L` y simplemente significa
**‚Äúllevar la serie un paso hacia atr√°s‚Äù**.

Por definici√≥n:

.. math::


   L y_t = y_{t-1}

y de manera general:

.. math::


   L^k y_t = y_{t-k}

Es decir, aplicar :math:`L` una vez es ir un periodo atr√°s, aplicarlo
:math:`k` veces es ir :math:`k` periodos atr√°s.

**Ejemplo sencillo**

Supongamos una serie :math:`y_t` con valores:

-  En :math:`t=5`, :math:`y_5 = 10`

-  Aplicamos :math:`L`: :math:`L y_5 = y_4`

-  Aplicamos :math:`L^2`: :math:`L^2 y_5 = y_3`

El operador rezago funciona como una ‚Äúm√°quina del tiempo‚Äù que desplaza
la serie hacia el pasado.

**Usando el rezago en modelos AR**

Un **AR(1)**:

.. math::


   y_t = \alpha + \phi_1 y_{t-1} + \varepsilon_t

se puede escribir con el operador rezago como:

.. math::


   y_t = \alpha + \phi_1 L y_t + \varepsilon_t

Reordenando:

.. math::


   (1 - \phi_1 L) y_t = \alpha + \varepsilon_t

**Generalizaci√≥n al AR(p)**

El modelo:

.. math::


   y_t = \alpha + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t

usando el operador rezago queda:

.. math::


   (1 - \phi_1 L - \phi_2 L^2 - \cdots - \phi_p L^p) y_t = \alpha + \varepsilon_t

**El polinomio caracter√≠stico**

La parte entre par√©ntesis se llama **polinomio caracter√≠stico**:

.. math::


   \Phi(L) = 1 - \phi_1 L - \phi_2 L^2 - \cdots - \phi_p L^p

La condici√≥n de estabilidad es que las **ra√≠ces de :math:`\Phi(L)` est√©n
fuera del c√≠rculo unitario**, es decir, que tengan m√≥dulo mayor que 1.

Esto garantiza que los choques :math:`\varepsilon_t` no se acumulen sino
que se disipen con el tiempo.

**Intuici√≥n final**

-  El operador :math:`L` es solo una forma compacta de escribir
   ‚Äúrezagos‚Äù.

-  Gracias a :math:`L`, podemos representar un modelo AR(p) como un
   polinomio.

-  Revisar las **ra√≠ces del polinomio** nos dice si la serie es
   **estable (estacionaria)** o si tiene una **ra√≠z unitaria** (paseo
   aleatorio) o incluso si **explota**.

üëâ As√≠, el operador rezago no es un ‚Äútruco raro‚Äù, sino una herramienta
matem√°tica que simplifica la escritura y el an√°lisis de modelos
autorregresivos.

C√≥mo determinar el orden p en un modelo AR(p)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Elegir el n√∫mero de rezagos :math:`p` es una de las decisiones m√°s
importantes en los modelos autorregresivos.

Existen tres enfoques principales: **ACF/PACF**, **criterios de
informaci√≥n** y **validaci√≥n de residuales**.

**1. An√°lisis visual con ACF y PACF**

-  **ACF (Funci√≥n de Autocorrelaci√≥n):**

   Muestra c√≥mo se correlaciona la serie con sus propios rezagos.

   En un AR(p), la ACF no corta bruscamente; en cambio, **decae de forma
   gradual** (exponencial u oscilante). Un modelo **AR(p)**
   (autorregresivo) se caracteriza por tener una **dependencia del
   pasado** que se **disipa gradualmente** en el tiempo. Por eso, en
   teor√≠a, la **ACF de un proceso AR(p)** **no corta bruscamente**, sino
   que **decae suavemente** (a veces de forma exponencial, a veces
   oscilando).

-  **PACF (Funci√≥n de Autocorrelaci√≥n Parcial):**

   Mide la correlaci√≥n entre :math:`y_t` y :math:`y_{t-k}` eliminando la
   influencia de los rezagos intermedios.

   En un AR(p), la PACF muestra un **corte brusco** despu√©s del rezago
   :math:`p`.

   Es decir: los primeros :math:`p` rezagos aparecen significativos, y
   los dem√°s son aproximadamente cero.

**Ejemplo:**

-  PACF con picos significativos en :math:`lag=1` y :math:`lag=2`, pero
   no despu√©s ‚Üí sugiere un AR(2).

-  ACF que decae lentamente confirma que el modelo es de tipo AR.

**C√≥mo se comporta la ACF en diferentes casos:**

+-------------------------+---------------------+----------------------+
| Tipo de proceso         | Patr√≥n en ACF       | Patr√≥n en PACF       |
+=========================+=====================+======================+
| **AR(1)**               | Decae               | Corte brusco en lag  |
|                         | exponencialmente u  | 1                    |
|                         | oscilando (seg√∫n    |                      |
|                         | signo de œÜ‚ÇÅ)        |                      |
+-------------------------+---------------------+----------------------+
| **AR(2)**               | Decaimiento suave u | Corte en lag 2       |
|                         | oscilante           |                      |
+-------------------------+---------------------+----------------------+
| **MA(q)**               | Corte brusco en lag | Decae lentamente     |
|                         | q                   |                      |
+-------------------------+---------------------+----------------------+
| **ARMA(p,q)**           | ACF y PACF decaen   | Ninguna corta        |
|                         | suavemente          | bruscamente          |
+-------------------------+---------------------+----------------------+
| **Ruido blanco**        | Sin autocorrelaci√≥n | Sin autocorrelaci√≥n  |
|                         | (todas ‚âà 0)         | (todas ‚âà 0)          |
+-------------------------+---------------------+----------------------+

**Casos especiales donde la ACF no ‚Äúdecae lentamente‚Äù**

1. **Cuando los coeficientes AR son negativos:**

   -  La ACF **oscila alrededor de cero** (patr√≥n de dientes de sierra)
      en lugar de decaer de manera mon√≥tona.

   -  Ejemplo: AR(1) con œÜ‚ÇÅ = ‚Äì0.7 ‚Üí alterna correlaciones positivas y
      negativas.

2. **Cuando hay ra√≠ces complejas en un AR(2):**

   -  La ACF muestra **ondas amortiguadas**: un patr√≥n oscilatorio que
      se aten√∫a con el tiempo.

   -  No es ‚Äúlento‚Äù en el sentido cl√°sico, pero sigue siendo un
      **decaimiento amortiguado**.

3. **Cuando el proceso no es estacionario:**

   -  Si :math:`|\phi| ‚â• 1`, la ACF **no decae** (permanece alta o
      diverge).

   -  Esto indica una **ra√≠z unitaria** (paseo aleatorio), no un proceso
      AR estacionario.

**2. Criterios de informaci√≥n (AIC, BIC)**

Se ajustan modelos con distintos valores de :math:`p` y se comparan
criterios estad√≠sticos:

-  **AIC (Akaike Information Criterion)**

-  **BIC (Bayesian Information Criterion)**

**Regla:** elegir el modelo que minimice estos valores.

-  El AIC suele preferir modelos m√°s grandes (menos penalizaci√≥n).

-  El BIC es m√°s estricto (prefiere modelos m√°s simples).

Esto permite refinar la elecci√≥n sugerida por ACF/PACF.

**3. Validaci√≥n de residuales**

Despu√©s de elegir :math:`p` con PACF o criterios de informaci√≥n:

1. Revisar los **residuales del modelo**: deben parecer **ruido
   blanco**.

   -  ACF de residuales: no debe mostrar autocorrelaci√≥n.

   -  Prueba de Ljung‚ÄìBox: no debe rechazar la hip√≥tesis de
      independencia.

2. Si los residuales muestran autocorrelaci√≥n ‚Üí probablemente falten
   rezagos, aumentar :math:`p`.

3. Si el modelo parece sobreajustado (par√°metros no significativos o
   :math:`p` demasiado grande) ‚Üí reducir :math:`p`.

**4. Reglas emp√≠ricas adicionales**

-  **Series cortas (n < 50):** conviene mantener :math:`p` peque√±o (ej.
   1‚Äì3).

-  **Series largas:** se puede probar valores mayores de :math:`p`, pero
   un l√≠mite pr√°ctico es :math:`\sqrt{n}` rezagos como m√°ximo a evaluar.

+---------------------+--------+-------------------+------------------+
| M√©todo              | Qu√©    | Patr√≥n esperado   | C√≥mo usarlo para |
|                     | ob     | en un AR(p)       | elegir p         |
|                     | servar |                   |                  |
+=====================+========+===================+==================+
| **ACF               | Corre  | Decae lentamente  | Confirma que la  |
| (Autocorrelaci√≥n)** | laci√≥n | (exponencial u    | serie es de tipo |
|                     | entre  | oscilante), no    | AR               |
|                     | :math  | corta bruscamente |                  |
|                     | :`y_t` |                   |                  |
|                     | y      |                   |                  |
|                     | r      |                   |                  |
|                     | ezagos |                   |                  |
+---------------------+--------+-------------------+------------------+
| **PACF              | Corre  | **Corte brusco en | El √∫ltimo rezago |
| (Autocorrelaci√≥n    | laci√≥n | el rezago p**     | significativo    |
| parcial)**          | d      | (los primeros p   | indica el valor  |
|                     | irecta | lags son          | de p             |
|                     | entre  | significativos,   |                  |
|                     | :math  | luego ‚âà 0)        |                  |
|                     | :`y_t` |                   |                  |
|                     | y      |                   |                  |
|                     | :ma    |                   |                  |
|                     | th:`y_ |                   |                  |
|                     | {t-k}` |                   |                  |
|                     | elim   |                   |                  |
|                     | inando |                   |                  |
|                     | r      |                   |                  |
|                     | ezagos |                   |                  |
|                     | inter  |                   |                  |
|                     | medios |                   |                  |
+---------------------+--------+-------------------+------------------+
| **Criterios de      | C      | Se comparan       | Elegir el modelo |
| informaci√≥n (AIC,   | alidad | distintos modelos | con menor        |
| BIC)**              | del    | AR(p)             | AIC/BIC; BIC     |
|                     | ajuste |                   | suele ser m√°s    |
|                     | pena   |                   | conservador      |
|                     | lizada |                   |                  |
|                     | por    |                   |                  |
|                     | compl  |                   |                  |
|                     | ejidad |                   |                  |
+---------------------+--------+-------------------+------------------+
| **Diagn√≥stico de    | ACF y  | Residuales deben  | Si hay           |
| residuales**        | p      | parecer **ruido   | autocorrelaci√≥n  |
|                     | ruebas | blanco**          | ‚Üí aumentar p; si |
|                     | estad√≠ |                   | hay sobreajuste  |
|                     | sticas |                   | ‚Üí reducir p      |
|                     | sobre  |                   |                  |
|                     | resi   |                   |                  |
|                     | duales |                   |                  |
+---------------------+--------+-------------------+------------------+
| **Reglas            | Lo     | p peque√±o en      | Limita el rango  |
| emp√≠ricas**         | ngitud | series cortas;    | de b√∫squeda para |
|                     | de la  | m√°ximo pr√°ctico ‚âà | p                |
|                     | serie  | :math:`\sqrt{n}`  |                  |
|                     | :ma    |                   |                  |
|                     | th:`n` |                   |                  |
+---------------------+--------+-------------------+------------------+

.. figure:: Ejemplos_AR.png
   :alt: Ejemplos_AR

   Ejemplos_AR

Pron√≥stico con modelos AR
~~~~~~~~~~~~~~~~~~~~~~~~~

El objetivo del pron√≥stico con un modelo autorregresivo AR(p) es estimar
el valor futuro de la serie usando sus propios rezagos recientes.
Partimos del modelo ajustado:

.. math::


   y_t = \alpha + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t

donde :math:`\varepsilon_t` es ruido blanco.

**1. Pron√≥stico a 1 paso adelante**

El pron√≥stico en :math:`t+1` dado lo observado hasta :math:`t` es la
esperanza condicional:

.. math::


   \hat y_{t+1\mid t} = \hat\alpha + \hat\phi_1 y_t + \hat\phi_2 y_{t-1} + \cdots + \hat\phi_p y_{t-p+1}

-  Es **lineal** en los √∫ltimos :math:`p` valores observados.

-  Si el modelo incluye media :math:`\mu` en vez de intercepto, puede
   escribirse como:

   .. math::


      \hat y_{t+1\mid t} = \hat\mu + \hat\phi_1 (y_t-\hat\mu) + \cdots + \hat\phi_p (y_{t-p+1}-\hat\mu)

**2. Pron√≥stico multi-paso (h pasos)**

Para :math:`h \ge 2` el pron√≥stico es **recursivo**: se sustituyen los
valores futuros desconocidos por sus pron√≥sticos previos.

Ejemplo AR(1):

.. math::


   y_t = \mu + \phi\, y_{t-1} + \varepsilon_t

.. math::


   \hat y_{t+1\mid t} = \mu + \phi (y_t - \mu),\quad
   \hat y_{t+2\mid t} = \mu + \phi (\hat y_{t+1\mid t} - \mu) = \mu + \phi^2 (y_t - \mu)

En general:

.. math::


   \hat y_{t+h\mid t} = \mu + \phi^h (y_t - \mu)

Para AR(p), el mismo principio aplica pero usando la ecuaci√≥n del
modelo:

-  Para construir :math:`\hat y_{t+h\mid t}` se usan
   :math:`y_{t},\dots,y_{t-p+1}` y, cuando haga falta,
   :math:`\hat y_{t+1\mid t},\dots,\hat y_{t+h-1\mid t}`.

**3. Incertidumbre del pron√≥stico e intervalos**

La varianza del error de pron√≥stico **crece con :math:`h`** y se
aproxima a la varianza incondicional del proceso cuando el modelo es
estable.

Representaci√≥n MA(:math:`\infty`):

.. math::


   y_t - \mu = \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j},\quad \psi_0=1

Varianza del error a :math:`h` pasos:

.. math::


   \operatorname{Var}\!\left(y_{t+h} - \hat y_{t+h\mid t}\right) = \sigma_\varepsilon^2 \sum_{j=0}^{h-1} \psi_j^2

Caso AR(1):

.. math::


   \operatorname{Var}\!\left(y_{t+h} - \hat y_{t+h\mid t}\right) = \sigma_\varepsilon^2 \frac{1-\phi^{2h}}{1-\phi^2}

Intervalo de pron√≥stico aproximado al nivel :math:`(1-\alpha)`:

.. math::


   \hat y_{t+h\mid t} \ \pm\ z_{1-\alpha/2}\, \sqrt{ \widehat{\operatorname{Var}}\!\left(y_{t+h} - \hat y_{t+h\mid t}\right) }

**4. Procedimiento pr√°ctico paso a paso**

1. **Preparaci√≥n**

   -  Asegurar estacionariedad en media y varianza.

   -  Remover tendencia y estacionalidad si existen (diferencias,
      desestacionalizaci√≥n, log).

2. **Identificaci√≥n de** :math:`p`

   -  Leer ACF y PACF.

   -  Comparar AIC y BIC en varios AR(p).

3. **Estimaci√≥n**

   -  Ajustar el AR(p) por m√≠nimos cuadrados o m√°xima verosimilitud.

   -  Verificar estabilidad (ra√≠ces fuera del c√≠rculo unitario).

4. **Diagn√≥stico**

   -  Residuales ~ ruido blanco (ACF/PACF de residuales, Ljung‚ÄìBox).

   -  Q‚ÄìQ plot si se requiere normalidad para inferencia.

5. **Pron√≥stico**

   -  Generar :math:`\hat y_{t+h\mid t}` de forma recursiva.

   -  Calcular intervalos de pron√≥stico con la varianza correspondiente.

6. **Evaluaci√≥n fuera de muestra**

   -  Backtesting con ventana **rodante** o **expansiva**.

   -  M√©tricas: MAE, RMSE, MAPE, MSE.

   -  Comparar con benchmarks simples: promedio, na√Øve, random walk,
      SES.

**5. Intuiciones √∫tiles**

-  A medida que :math:`h` aumenta, el pron√≥stico **converge a la media**
   del proceso estacionario.

-  Un :math:`\phi` cercano a 1 implica **persistencia alta** y, por
   tanto, **intervalos m√°s anchos** para horizontes largos.

-  En AR(2) con ra√≠ces complejas, los pron√≥sticos presentan
   **oscilaciones amortiguadas** hacia la media.

-  Si hay autocorrelaci√≥n remanente en residuales, el modelo tiende a
   **subestimar la incertidumbre** del pron√≥stico.

Pron√≥stico in-sample y out-of-sample
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cuando ajustamos un modelo AR(p), podemos evaluar su capacidad de
pron√≥stico de dos formas distintas:

**in-sample (dentro de la muestra)** y **out-of-sample (fuera de la
muestra)**.

**1. Pron√≥stico in-sample (dentro de la muestra)**

Corresponde a los valores **ya observados** que el modelo intenta
**reconstruir o explicar** dentro del periodo usado para entrenar el
modelo.

.. math::


   \hat y_t = \hat\alpha + \hat\phi_1 y_{t-1} + \hat\phi_2 y_{t-2} + \cdots + \hat\phi_p y_{t-p}

-  Se calculan los valores ajustados **usando los mismos datos del
   entrenamiento**.

-  Permiten evaluar **qu√© tan bien el modelo reproduce la din√°mica del
   pasado**.

**Prop√≥sito:**

Evaluar el **ajuste interno** del modelo (goodness of fit).

**Indicadores comunes:**

-  :math:`R^2` o coeficiente de determinaci√≥n.

-  Error medio cuadr√°tico (MSE) o ra√≠z del error cuadr√°tico medio
   (RMSE).

-  An√°lisis visual: comparaci√≥n entre serie observada y serie ajustada.

**Limitaci√≥n:**

Un modelo puede tener un ajuste excelente in-sample y, aun as√≠, fallar
al predecir el futuro ‚Üí riesgo de **sobreajuste (overfitting)**.

**2. Pron√≥stico out-of-sample (fuera de la muestra)**

Corresponde a **valores futuros no usados en la estimaci√≥n** del modelo.

Se usa para evaluar la **capacidad predictiva real**.

.. math::


   \hat y_{t+h\mid t} = \hat\alpha + \hat\phi_1 y_{t+h-1} + \hat\phi_2 y_{t+h-2} + \cdots + \hat\phi_p y_{t+h-p}

-  Se realiza sobre un **conjunto de prueba (test)** separado del
   entrenamiento.

-  Los valores pasados de :math:`y_t` pueden provenir de **datos reales
   o de pron√≥sticos previos (pron√≥stico recursivo).**

-  Mide qu√© tan bien el modelo generaliza a datos nuevos.

**Prop√≥sito:**

Evaluar la **capacidad de pron√≥stico genuina**, no el ajuste hist√≥rico.

**M√©tricas comunes:**

-  RMSE (Root Mean Squared Error)

-  MSE (Mean Squared Error)

-  MAE (Mean Absolute Error)

-  MAPE (Error porcentual absoluto medio)

**3. Ejemplo conceptual**

Suponemos que tenemos una serie de 120 meses.

+---------------+---------------+------------------------------------+
| Per√≠odo       | Uso           | Descripci√≥n                        |
+===============+===============+====================================+
| Mes 1 ‚Äì 100   | Entrenamiento | Se usa para estimar el modelo      |
|               |               | (in-sample)                        |
+---------------+---------------+------------------------------------+
| Mes 101 ‚Äì 120 | Prueba        | Se usa para evaluar pron√≥stico     |
|               |               | futuro (out-of-sample)             |
+---------------+---------------+------------------------------------+

1. Ajustas el modelo AR(p) con los primeros 100 meses.

2. Calculas los valores ajustados :math:`\hat y_t` ‚Üí **in-sample**.

3. Realizas pron√≥sticos recursivos para los meses 101 a 120 ‚Üí
   **out-of-sample**.

4. Comparas con los valores reales :math:`y_{101}, \dots, y_{120}`.

**4. Evaluaci√≥n conjunta**

+--------------------+--------------+-----------+---------+----------+
| Tipo de pron√≥stico | Datos usados | Prop√≥sito | Riesgos | M√©tricas |
+====================+==============+===========+=========+==========+
| **In-sample**      | Datos de     | Verificar | Sobr    | R¬≤, MSE, |
|                    | e            | ajuste    | eajuste | RMSE,    |
|                    | ntrenamiento | interno   | (modelo | MAE,     |
|                    |              |           | de      | MAPE     |
|                    |              |           | masiado |          |
|                    |              |           | co      |          |
|                    |              |           | mplejo) |          |
+--------------------+--------------+-----------+---------+----------+
| **Out-of-sample**  | Datos de     | Evaluar   | V       | R¬≤, MSE, |
|                    | prueba (no   | capacidad | arianza | RMSE,    |
|                    | vistos)      | p         | alta o  | MAE,     |
|                    |              | redictiva | mala    | MAPE     |
|                    |              | real      | general |          |
|                    |              |           | izaci√≥n |          |
+--------------------+--------------+-----------+---------+----------+

**5. Buenas pr√°cticas**

-  Siempre separar los datos en **entrenamiento y prueba** (por ejemplo,
   80/20).

-  Validar con **pron√≥stico recursivo**.

-  Un modelo √∫til no es el que mejor ajusta el pasado, sino el que
   **predice mejor el futuro**.

-  Comparar los errores out-of-sample con un **modelo na√Øve** (por
   ejemplo, :math:`y_{t+1} = y_t`).

   Si el AR(p) no mejora al modelo na√Øve ‚Üí no agrega valor predictivo.

**6. Visualizaci√≥n t√≠pica**

-  Gr√°fico de la serie observada, con:

   -  Datos reales (entrenamiento + prueba).

   -  Pron√≥stico in-sample (ajuste).

   -  Pron√≥stico out-of-sample (proyecci√≥n futura).

   -  Intervalo de confianza.

**7. Conclusi√≥n**

-  **In-sample:** mide qu√© tan bien el modelo explica el pasado.

-  **Out-of-sample:** mide qu√© tan bien el modelo predice el futuro.

-  Ambos deben analizarse juntos:

   -  Buen ajuste in-sample + mal desempe√±o out-of-sample ‚Üí
      **sobreajuste**.

   -  Mal ajuste in-sample + buen desempe√±o out-of-sample ‚Üí **modelo m√°s
      robusto**.

..

   En series de tiempo, el verdadero test de un modelo AR no es qu√© tan
   bien ajusta la historia, sino **qu√© tan cre√≠blemente anticipa lo que
   a√∫n no ha ocurrido.**

¬øIntercepto o media en un modelo AR(p)?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**1. Forma con intercepto** Un modelo AR(p) puede escribirse como:

.. math::


   y_t = \alpha + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t

Aqu√≠, :math:`\alpha` es el **intercepto**.

Esta forma es √∫til cuando se est√° estimando directamente por regresi√≥n
(ej. m√≠nimos cuadrados) porque se trata como una constante m√°s.

**2. Forma centrada en la media**

| Bajo estacionariedad, la serie tiene una **media constante**
  :math:`\mu`.
| El mismo modelo puede reescribirse como:

.. math::


   y_t - \mu = \phi_1 (y_{t-1} - \mu) + \phi_2 (y_{t-2} - \mu) + \cdots + \phi_p (y_{t-p} - \mu) + \varepsilon_t

donde :math:`\mu` es la media de la serie.

En este caso **no se incluye** :math:`\alpha` **expl√≠citamente**, porque
ya est√° absorbida en la media.

**3. Relaci√≥n entre intercepto y media**

Si trabajas con intercepto:

.. math::


   \mu = \frac{\alpha}{1 - \phi_1 - \phi_2 - \cdots - \phi_p}

Siempre que :math:`1 - \phi_1 - \cdots - \phi_p \neq 0` (condici√≥n de
estacionariedad).

**4. ¬øCu√°ndo usar cada forma?**

-  **Intercepto** (:math:`\alpha`):

   -  Cuando el modelo se estima con m√©todos de regresi√≥n lineal
      directamente.

   -  Es la forma m√°s com√∫n en la pr√°ctica computacional.

   -  Los paquetes de software (``statsmodels``, ``R``, etc.) suelen
      reportar :math:`\alpha`.

-  **Media** (:math:`\mu`):

   -  Cuando quieres interpretar el modelo en t√©rminos de la **tendencia
      de largo plazo**.

   -  √ötil para entender hacia d√≥nde **converge el pron√≥stico** cuando
      el horizonte :math:`h \to \infty` (siempre converge a
      :math:`\mu`).

   -  En textos te√≥ricos se usa porque facilita derivar propiedades
      (media, varianza, covarianza).

**5. Intuici√≥n**

-  El **intercepto** :math:`\alpha` es una ‚Äúconstante de ajuste‚Äù en la
   ecuaci√≥n de regresi√≥n.

-  La **media** :math:`\mu` es el ‚Äúpunto de equilibrio‚Äù del proceso: el
   valor al que los pron√≥sticos tienden con el tiempo.

Estimaci√≥n de par√°metros en un modelo AR(p)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Existen tres m√©todos principales para estimar los par√°metros
:math:`\alpha, \phi_1, \dots, \phi_p`.

**1. Estimaci√≥n por M√≠nimos Cuadrados (OLS)**

-  Se reescribe el AR(p) como una regresi√≥n lineal m√∫ltiple:

.. math::


   y_t = \alpha + \phi_1 y_{t-1} + \cdots + \phi_p y_{t-p} + \varepsilon_t

-  Se estima minimizando la suma de cuadrados de los residuos:

.. math::


   \min_{\alpha, \phi_1,\dots,\phi_p} \sum_{t=p+1}^n \hat\varepsilon_t^2

-  Es equivalente a un modelo de regresi√≥n est√°ndar con :math:`y_t` como
   variable dependiente y sus rezagos como explicativas.

**Ventajas:**

-  Sencillo y directo.

-  Bien implementado en cualquier software estad√≠stico.

-  Consistente y eficiente si :math:`\varepsilon_t` es ruido blanco
   gaussiano.

**Limitaci√≥n:**

-  Puede no ser tan eficiente si los residuos no son normales.

**2. Estimaci√≥n por M√°xima Verosimilitud (MLE)**

-  Asume que :math:`\varepsilon_t \sim N(0,\sigma_\varepsilon^2)`.

-  La **funci√≥n de verosimilitud** es el producto de las densidades
   normales de los residuos.

-  En pr√°ctica, se trabaja con la **log-verosimilitud**:

.. math::


   \ell(\theta) = -\frac{n}{2}\log(2\pi\sigma_\varepsilon^2) - \frac{1}{2\sigma_\varepsilon^2}\sum_{t=p+1}^n \hat\varepsilon_t^2

-  Maximizar :math:`\ell(\theta)` es **equivalente a minimizar la suma
   de cuadrados de los residuos** si se asume normalidad.

-  Por eso, en un AR puro, el MLE y OLS producen estimadores muy
   similares.

**Ventajas:**

-  Permite construir intervalos de confianza y pruebas de hip√≥tesis bajo
   supuestos normales.

-  Es la base para comparar modelos usando AIC/BIC.

**Limitaci√≥n:**

-  Requiere normalidad de los errores para ser eficiente.

**3. Estimaci√≥n por Yule‚ÄìWalker**

-  Se basa en las **ecuaciones de autocorrelaci√≥n** del proceso AR(p):

.. math::


   \rho_k = \phi_1 \rho_{k-1} + \phi_2 \rho_{k-2} + \cdots + \phi_p \rho_{k-p}, \quad k=1,\dots,p

-  Estas forman un sistema lineal que relaciona los coeficientes
   :math:`\phi_i` con las autocorrelaciones muestrales
   :math:`\hat\rho_k`.

-  Resolviendo el sistema se obtienen los estimadores de Yule‚ÄìWalker.

**Ventajas:**

-  F√°cil de calcular.

-  √ötil como estimador inicial para otros m√©todos (ej. en algoritmos
   iterativos).

-  R√°pido computacionalmente.

**Limitaci√≥n:**

-  Puede ser menos eficiente que OLS/MLE en muestras peque√±as.

-  Depende fuertemente de la calidad de las estimaciones de
   autocorrelaci√≥n.

**Comparaci√≥n de los m√©todos**

+---------------+----------------------+------------------+-----------+
| M√©todo        | Supuestos clave      | Ventajas         | Lim       |
|               |                      | principales      | itaciones |
+===============+======================+==================+===========+
| **OLS**       | Errores no           | Sencillo,        | Menos     |
|               | correlacionados      | intuitivo        | eficiente |
|               |                      |                  | si no hay |
|               |                      |                  | n         |
|               |                      |                  | ormalidad |
+---------------+----------------------+------------------+-----------+
| **MLE**       | Errores normales iid | Permite          | Computaci |
|               |                      | inferencia       | onalmente |
|               |                      | estad√≠stica      | m√°s       |
|               |                      | (intervalos,     | exigente  |
|               |                      | tests) y         |           |
|               |                      | selecci√≥n por    |           |
|               |                      | AIC/BIC          |           |
+---------------+----------------------+------------------+-----------+
| **            | Estacionariedad      | R√°pido, √∫til     | Menos     |
| Yule‚ÄìWalker** |                      | para             | eficiente |
|               |                      | inicializaci√≥n   | en        |
|               |                      |                  | muestras  |
|               |                      |                  | peque√±as  |
+---------------+----------------------+------------------+-----------+

Evaluaci√≥n de la significancia de los par√°metros en un modelo AR(p)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Una vez estimado el modelo:

.. math::


   y_t = \alpha + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t

cada par√°metro :math:`\phi_i` y el intercepto :math:`\alpha` tienen un
**estimador** :math:`\hat\phi_i` y un **error est√°ndar**
:math:`SE(\hat\phi_i)`.

**1. Prueba t individual**

La forma m√°s com√∫n de evaluar la significancia de cada par√°metro es
mediante la **prueba z**:

.. math::


   z_i = \frac{\hat\phi_i}{SE(\hat\phi_i)}

**Hip√≥tesis:**

-  :math:`H_0`: :math:`\phi_i = 0` (el rezago :math:`i` no tiene efecto
   significativo)

-  :math:`H_1`: :math:`\phi_i \neq 0` (el rezago s√≠ tiene efecto)

**Criterio de decisi√≥n:**

-  Si :math:`|z_i| > z_{\text{cr√≠tico}}` o el valor p < 0,05 ‚Üí se
   rechaza :math:`H_0`.

-  Es decir, el coeficiente :math:`\phi_i` es **significativo**.

-  Como referencia:

   -  Si :math:`|z| > 1.96` ‚Üí significativo al 5%

   -  Si :math:`|z| > 2.58` ‚Üí significativo al 1%

**Interpretaci√≥n:**

-  Un par√°metro significativo indica que ese rezago **aporta informaci√≥n
   predictiva** sobre :math:`y_t`.

-  Un par√°metro no significativo puede eliminarse para simplificar el
   modelo.

**2. Error est√°ndar y significancia pr√°ctica**

Aunque un coeficiente sea estad√≠sticamente significativo (valor p <
0.05), tambi√©n se eval√∫a su **magnitud**:

-  Si :math:`\phi_i` es muy peque√±o, su influencia pr√°ctica puede ser
   m√≠nima.

-  En modelos con muchos rezagos, eliminar coeficientes peque√±os y no
   significativos mejora la parsimonia.

**3. Prueba conjunta (Wald o F)**

Tambi√©n se puede evaluar si **varios par√°metros a la vez** son iguales a
cero:

.. math::


   H_0: \phi_1 = \phi_2 = \cdots = \phi_p = 0

Esto se puede hacer con una **prueba F (en OLS)** o **Wald test (en
MLE)**.

Si se rechaza :math:`H_0`, el conjunto de rezagos aporta informaci√≥n
significativa al modelo.

**4. Valores p y tabla resumen**

Los paquetes estad√≠sticos (por ejemplo, ``statsmodels`` en Python)
entregan una tabla con:

============== =========== ============== ======= ======= =============
Par√°metro      Coeficiente Error est√°ndar Valor t Valor p Significancia
============== =========== ============== ======= ======= =============
:math:`\alpha` 0.502       0.091          5.49    0.000   \**\*
:math:`\phi_1` 0.634       0.084          7.56    0.000   \**\*
:math:`\phi_2` -0.213      0.092          -2.31   0.022   \*\*
‚Ä¶              ‚Ä¶           ‚Ä¶              ‚Ä¶       ‚Ä¶       ‚Ä¶
============== =========== ============== ======= ======= =============

Los asteriscos indican niveles de significancia:

-  ``***`` p < 0.01 (muy significativo)

-  ``**`` p < 0.05 (significativo)

-  ``*`` p < 0.10 (marginal)

-  ``ns`` no significativo

**5. Cu√°ndo preocuparse por la significancia**

-  Si muchos coeficientes no son significativos ‚Üí probablemente
   :math:`p` es demasiado grande (sobreajuste).

-  Si todos son significativos ‚Üí el modelo capta bien la din√°mica.

-  Si solo los primeros rezagos son significativos ‚Üí se puede reducir el
   orden a ese nivel.

**6. Precauci√≥n: correlaci√≥n entre rezagos**

En modelos con rezagos cercanos (por ejemplo, AR(6)), puede existir
**colinealidad** entre los valores pasados.

Esto aumenta los errores est√°ndar y puede hacer que algunos coeficientes
**no parezcan significativos**, aunque el modelo global s√≠ lo sea.

Por eso tambi√©n se usa el **AIC/BIC** y el diagn√≥stico de **residuales**
para validar el modelo completo, no solo los valores p individuales.

**7. Resumen pr√°ctico**

+-------+---------------------+------------------+---------------------+
| Paso  | Qu√© se eval√∫a       | Herramienta      | Interpretaci√≥n      |
+=======+=====================+==================+=====================+
| 1     | Significancia       | Prueba t, valor  | Si p < 0.05, el     |
|       | individual de       | p                | rezago es relevante |
|       | :math:`\phi_i`      |                  |                     |
+-------+---------------------+------------------+---------------------+
| 2     | Significancia       | Prueba F o Wald  | Eval√∫a si los       |
|       | conjunta de varios  |                  | rezagos en conjunto |
|       | :math:`\phi_i`      |                  | explican la serie   |
+-------+---------------------+------------------+---------------------+
| 3     | Parsimonia del      | AIC/BIC y        | Confirmar que no    |
|       | modelo              | residuos         | hay sobreajuste     |
+-------+---------------------+------------------+---------------------+
| 4     | Multicolinealidad   | Revisar          | Si alta, puede      |
|       |                     | correlaciones    | afectar las t       |
|       |                     | entre rezagos    |                     |
+-------+---------------------+------------------+---------------------+

Intervalos de confianza en los modelos AR
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cuando ajustamos un modelo autorregresivo con ``statsmodels``, en la
tabla de resultados aparecen dos columnas adicionales Las columnas
**[0.025, 0.975]** representan los **l√≠mites inferior y superior** del
**intervalo de confianza al 95%** para cada par√°metro estimado.

**1. ¬øQu√© es un intervalo de confianza?**

Un **intervalo de confianza (IC)** indica el rango de valores dentro del
cual se espera que se encuentre el **valor real del par√°metro
poblacional**, con una probabilidad determinada (habitualmente 95%).

Formalmente, para cada par√°metro :math:`\hat{\phi_i}`:

.. math::


   IC_{95\%} = \hat{\phi_i} \pm 1.96 \times SE(\hat{\phi_i})

donde:

-  :math:`\hat{\phi_i}` ‚Üí coeficiente estimado,

-  :math:`SE(\hat{\phi_i})` ‚Üí error est√°ndar del coeficiente,

-  :math:`1.96` ‚Üí valor cr√≠tico de la distribuci√≥n normal est√°ndar para
   un 95% de confianza.

**2. Interpretaci√≥n pr√°ctica**

-  El **intervalo [0.025, 0.975]** muestra el rango en el que se
   encuentra el valor verdadero del par√°metro con **95% de confianza**.

-  Si el intervalo **no contiene el valor 0**, se concluye que el
   par√°metro es **estad√≠sticamente significativo**.

-  Si el intervalo **incluye 0**, no se puede afirmar que el efecto sea
   distinto de cero.

**3. Interpretaci√≥n geom√©trica**

-  El punto central del intervalo es el **coeficiente estimado**.

-  El ancho del intervalo depende del **error est√°ndar**:

   -  Intervalo estrecho ‚Üí estimaci√≥n precisa.

   -  Intervalo ancho ‚Üí estimaci√≥n incierta (poca informaci√≥n en los
      datos).
