RegresiÃ³n LogÃ­stica
-------------------

**Un mÃ©todo de clasificaciÃ³n binaria:**

La regresiÃ³n logÃ­stica, tambiÃ©n conocida como Logit, es un clasificador
binario utilizado para modelar variables categÃ³ricas, particularmente
variables binarias o dicotÃ³micas. La variable dependiente puede tener
solo dos valores: 0 o 1, representando â€œsÃ­â€ y â€œnoâ€, â€œÃ©xitoâ€ y â€œfracasoâ€,
o â€œverdaderoâ€ y â€œfalsoâ€. Estas asignaciones son arbitrarias y se aplican
a caracterÃ­sticas cualitativas. La regresiÃ³n logÃ­stica clasifica las
observaciones en estas dos categorÃ­as segÃºn la probabilidad estimada en
el modelo. Por tanto, no se predice directamente si una observaciÃ³n es 1
o 0, sino la probabilidad de que pertenezca a la categorÃ­a 1.

La variable respuesta :math:`ğ‘¦` una variable aleatoria Bernoulli con la
siguiente distribuciÃ³n de probabilidad:

-  :math:`y=1` con una probabilidad :math:`p`.

-  :math:`y=0` con una probabilidad :math:`p-1`

El valor esperado de :math:`y` es :math:`p`, es decir, el valor esperado
es la probabilidad de que la variable :math:`y` tenga el valor de 1.

Dado que la variable respuesta :math:`y` es binaria, se utiliza una
funciÃ³n no lineal que puede ser creciente o decreciente en forma de â€œSâ€
o â€œSâ€ invertida. La funciÃ³n mÃ¡s utilizada es la funciÃ³n logÃ­stica:

.. math::  \sigma(z)= \frac{1}{1+e^z} 

â€‹

donde :math:`z` es una combinaciÃ³n lineal de las caracterÃ­sticas de
entrada.

La funciÃ³n sigmoide transforma cualquier valor real en un valor entre 0
y 1, lo que puede interpretarse como una probabilidad.

En el denominador de esta ecuaciÃ³n aparece la ecuaciÃ³n de regresiÃ³n
lineal, pero tiene una transformaciÃ³n. Con esta funciÃ³n se garantiza que
los valores predichos ven entre cero y uno, tal como se supone que lo
hacen las probabilidades.

.. figure:: S.JPG
   :alt: S

   S

En la regresiÃ³n logÃ­stica, el umbral (threshold) es el valor que se
utiliza para decidir a quÃ© clase se asigna una observaciÃ³n basada en la
probabilidad predicha. La regresiÃ³n logÃ­stica calcula la probabilidad de
que una observaciÃ³n pertenezca a la clase positiva (por ejemplo, clase
1). Por defecto, este umbral se establece en 0.5, lo que significa que:

-  Si la probabilidad predicha es mayor o igual a 0,5, la observaciÃ³n se
   clasifica como clase 1.

-  Si la probabilidad predicha es menor a 0,5, la observaciÃ³n se
   clasifica como clase 0.

El valor predeterminado de 0,5 puede no ser Ã³ptimo para todos los
problemas. Dependiendo del contexto, puede ser necesario ajustar el
umbral para equilibrar mejor las tasas de falsos positivos y falsos
negativos, especialmente si las clases estÃ¡n desbalanceadas o si las
consecuencias de errores de clasificaciÃ³n son diferentes.

.. figure:: Regression.JPG
   :alt: Regression

   Regression

InterpretaciÃ³n en los coeficientes:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

La interpretaciÃ³n de los coeficientes :math:`\beta` en la regresiÃ³n
logÃ­stica es diferente a la de los modelos de regresiÃ³n lineal.

Dado que:

.. math::  log \left(\frac{p}{1-p}\right) = \beta_0+\beta_1 \times X_1 + ... + \beta_n \times X_n 

Exponencial a ambos lados para obtener:

.. math::  \frac{p}{1-p} = e^{\beta_0+\beta_1 \times X_1 + ... + \beta_n \times X_n}

Se deduca que, para el modelo de regresiÃ³n:

.. math::  p = \frac{e^{\beta_0+\beta_1 \times X_1 + ... + \beta_n \times X_n}}{1+\beta_0+\beta_1 \times X_1 + ... + \beta_n \times X_n} 

La interpretaciÃ³n de los coeficientes se hace a travÃ©s de los odds
ratio, aplicando la exponencial a los resultados estimados. Para una
variable regresora :math:`ğ‘‹_ğ‘—`, el odds ratio es igual a
:math:`e^{\beta_j}`. Este valor representa el aumento estimado de la
probabilidad de â€œÃ©xitoâ€ (:math:`p`), asociado a un cambio unitario en el
valor de la variable predictora :math:`ğ‘‹_ğ‘—`, suponiendo que las demÃ¡s
variables permanecen constantes.

**Ejemplo de interpretaciÃ³n:**

Supongamos que el coeficiente estimado :math:`\beta_1=0,1`. Esto implica
que un aumento unitario en la variable :math:`ğ‘‹_1` incrementa en un 10%
la probabilidad de obtener â€œÃ©xitoâ€. Si :math:`ğ‘‹_1` aumenta en 20
unidades, entonces :math:`e^{0,1\times20}= 7,39`, lo que indica que las
probabilidades aumentan aproximadamente 7,39 veces.

Si en este ejemplo el coeficiente :math:`\beta_1` fuera negativo, por
ejemplo :math:`\beta_1=âˆ’0,1`, entonces el odds ratio serÃ­a menor que 1,
especÃ­ficamente :math:`e^{-0,1}= 0,91`. Esto se interpreta como que un
aumento unitario en la variable :math:`ğ‘‹_1` reduce la probabilidad de
Ã©xito en aproximadamente un 9%.

**Diferencia porcentual:**

Otra forma de interpretar los coeficientes es a travÃ©s de la diferencia
porcentual. Con la ecuaciÃ³n anterior, podemos determinar el efecto que
tienen los coeficientes :math:`\beta` negativos sobre la variable
resultado. Recuerde que a los coeficientes :math:`\beta` se les aplica
la funciÃ³n exponencial, que siempre toma valores positivos. Sin embargo,
con valores negativos, el resultado de la exponencial es menor que la
unidad.

-  Si :math:`\beta` es negativo, entonces :math:`e^{\beta}` es menor que
   1. El resultado es el porcentaje en el que disminuye la variable
   resultado :math:`ğ‘` para un aumento unitario en la variable
   :math:`ğ‘‹_ğ‘—`.

-  Si :math:`\beta` es positivo, entonces :math:`e^{\beta}` es mayor que
   1. El resultado es el porcentaje en el que aumenta la variable
   resultado :math:`ğ‘` para un aumento unitario en la variable
   :math:`ğ‘‹_ğ‘—`.

Por ejemplo, si :math:`\beta_1=0,1`, entonces :math:`e^{0,10}=1,105`.
Esto indica que un aumento unitario en :math:`ğ‘‹_1` incrementa la
probabilidad de Ã©xito en aproximadamente un 10,5%. Si
:math:`\beta_1=-0,1`, entonces :math:`e^{-0,10}=0,905`, lo que significa
que un aumento unitario en :math:`ğ‘‹_1` reduce la probabilidad de Ã©xito
en aproximadamente un 9,5%.

Ajuste en scikit-learn:
~~~~~~~~~~~~~~~~~~~~~~~

Durante el ajuste de un modelo de regresiÃ³n logÃ­stica, se utiliza un
algoritmo de optimizaciÃ³n (como el mÃ©todo de mÃ¡xima verosimilitud) para
encontrar los coeficientes del modelo que minimicen la funciÃ³n de
pÃ©rdida. Este proceso es iterativo y se repite hasta que se alcanza la
convergencia (es decir, hasta que los cambios en los coeficientes sean
muy pequeÃ±os entre una iteraciÃ³n y la siguiente) o hasta que se alcanza
el nÃºmero mÃ¡ximo de iteraciones especificado por ``max_iter``.

El valor predeterminado de ``max_iter`` es 100, lo que significa que el
algoritmo de optimizaciÃ³n realizarÃ¡ como mÃ¡ximo 100 iteraciones para
intentar encontrar la soluciÃ³n Ã³ptima. Si el algoritmo no ha convergido
despuÃ©s de 100 iteraciones, puede ser necesario aumentar este valor.

El objetivo es encontrar la curva que maximice la verosimilitud. En la
prÃ¡ctica, usualmente encontramos la curva Ã³ptima utilizando Gradiente
descendente.

Desventajas de la regresiÃ³n logÃ­stica:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

La regresiÃ³n logÃ­stica es una tÃ©cnica popular y efectiva para problemas
de clasificaciÃ³n binaria. Sin embargo, presenta varias desventajas y
limitaciones que deben tenerse en cuenta:

**1. RelaciÃ³n lineal entre las variables independientes y el Logit:**

-  La regresiÃ³n logÃ­stica asume una relaciÃ³n lineal entre las variables
   independientes y el logit de la variable dependiente. Si la relaciÃ³n
   real entre las variables no es lineal, el modelo puede no ajustarse
   bien a los datos.

**2. Limitada a problemas binarios:**

-  La regresiÃ³n logÃ­stica bÃ¡sica se utiliza para problemas de
   clasificaciÃ³n binaria. Aunque existen extensiones como la regresiÃ³n
   logÃ­stica multinomial para mÃ¡s de dos clases, estas pueden ser mÃ¡s
   complejas y no siempre proporcionan los mejores resultados en
   comparaciÃ³n con otros mÃ©todos.

**3. Sensibilidad a outliers:**

-  La regresiÃ³n logÃ­stica puede ser sensible a los valores atÃ­picos
   (outliers). Estos valores extremos pueden afectar significativamente
   la estimaciÃ³n de los coeficientes y, en consecuencia, el rendimiento
   del modelo.

**4. Suposiciones sobre las variables independientes:**

-  La regresiÃ³n logÃ­stica asume que las variables independientes no
   estÃ¡n fuertemente correlacionadas entre sÃ­ (no hay
   multicolinealidad). La presencia de multicolinealidad puede inflar
   las varianzas de los coeficientes estimados y hacer que el modelo sea
   inestable.

**5. No captura relaciones complejas:**

-  La regresiÃ³n logÃ­stica no es adecuada para capturar relaciones no
   lineales o interacciones complejas entre las variables. MÃ©todos como
   los Ãrboles de DecisiÃ³n, las MÃ¡quinas de Vectores de Soporte o las
   redes neuronales pueden manejar mejor estas relaciones.

**6. Rendimiento en conjuntos de datos desbalanceados:**

-  La regresiÃ³n logÃ­stica puede tener problemas al manejar conjuntos de
   datos desbalanceados, donde una clase es mucho mÃ¡s frecuente que la
   otra. En estos casos, el modelo puede sesgarse hacia la clase
   mayoritaria, ignorando la clase minoritaria.

**7. Escalabilidad:**

-  En conjuntos de datos muy grandes, la regresiÃ³n logÃ­stica puede
   volverse computacionalmente costosa y lenta, especialmente si se
   requiere un ajuste fino de hiperparÃ¡metros.

**8. InterpretaciÃ³n de coeficientes en modelos complejos:**

-  Aunque los coeficientes de la regresiÃ³n logÃ­stica son interpretables,
   en modelos con muchas variables independientes, la interpretaciÃ³n
   puede volverse complicada y menos intuitiva.

.. figure:: No_S.JPG
   :alt: No_S

   No_S
