츼rboles de decisi칩n
-------------------

Los 치rboles de decisi칩n son un m칠todo de Machine Learning utilizado
tanto para clasificaci칩n como para regresi칩n. Estos algoritmos son
capaces de ajustar conjuntos de datos complejos y se caracterizan por su
capacidad de generar modelos f치ciles de interpretar y visualizar.

**Conceptos clave:**

-  **Nodo ra칤z:** El nodo superior del 치rbol que contiene toda la
   poblaci칩n de datos. Desde aqu칤 se inician las divisiones.

-  **Nodos internos:** Representan decisiones basadas en el valor de
   ciertos atributos.

-  **Hojas:** Representan la salida del modelo (clase en clasificaci칩n o
   valor en regresi칩n).

-  **Ramas:** Conectan nodos y representan el resultado de una prueba
   aplicada en un nodo.

**Construcci칩n del 츼rbol:**

La construcci칩n de un 치rbol de decisi칩n implica los siguientes pasos:

-  **Selecci칩n de la caracter칤stica de divisi칩n:** Se selecciona la
   caracter칤stica que mejor divide el conjunto de datos utilizando
   criterios como el 칤ndice de Gini, la entrop칤a o la ganancia de
   informaci칩n.

-  **Divisi칩n del nodo:** Dividir el nodo actual en dos o m치s
   subconjuntos basados en el valor de la caracter칤stica seleccionada.

-  **Repetici칩n:** Repetir el proceso de selecci칩n de caracter칤sticas y
   divisi칩n hasta que se cumpla un criterio de parada, como alcanzar una
   profundidad m치xima o un n칰mero m칤nimo de muestras en un nodo.

**Ventajas:**

-  **Interpretabilidad:** Son f치ciles de entender y visualizar.

-  **Preparaci칩n m칤nima de datos:** No requieren escalado o centrado de
   caracter칤sticas.

-  **Flexibilidad:** Pueden manejar tanto variables categ칩ricas como
   num칠ricas.

**Desventajas:**

-  **Sobreajuste:** Tienden a sobreajustar los datos de entrenamiento si
   no se podan adecuadamente.

-  **Inestabilidad:** Peque침as variaciones en los datos pueden producir
   치rboles completamente diferentes.

-  **Sesgo en caracter칤sticas categ칩ricas con muchos niveles:** Pueden
   preferir caracter칤sticas con muchos niveles, lo que puede llevar a
   divisiones menos significativas.

.. figure:: DecisionTree.png
   :alt: DecisionTree

   DecisionTree

.. figure:: DecisionTree-Ejemplo.png
   :alt: DecisionTree-Ejemplo

   DecisionTree-Ejemplo

Metodolog칤a:
~~~~~~~~~~~~

Los 치rboles de decisi칩n para clasificaci칩n realizan la clasificaci칩n en
dos o m치s variables discretas. La metodolog칤a de los 치rboles de
clasificaci칩n se basa en dividir el espacio de las variables de entrada
con el objetivo de separarlo en regiones de elementos que pertenecen a
una clase particular. Este proceso es recursivo y el 치rbol crece a
medida que se realizan m치s particiones, lo que se llama profundidad.

**Medidas de Impureza:**

**1. Impureza de Gini:** Mide la probabilidad de que un elemento
seleccionado al azar sea clasificado incorrectamente. La impureza de
Gini se calcula como:

.. math::  G_i = 1- \sum_{k=1}^n{p_{i,k}^2} 

Donde :math:`p_{i,k}` es la proporci칩n de elementos de la clase k entre
los elementos en el nodo i.

-  Gini < 0,1: Excelente. El nodo es casi puro.

-  Gini entre 0,1 y 0,3: Bueno. El nodo tiene una pureza razonablemente
   alta.

-  Gini entre 0,3 y 0,4: Aceptable. El nodo tiene cierta impureza, pero
   a칰n puede ser 칰til.

-  Gini > 0,4: Pobre. El nodo es bastante impuro y podr칤a no ser muy
   칰til para la clasificaci칩n.

**Valor m치ximo del 칤ndice de Gini:**

El valor m치ximo del 칤ndice de Gini depende del n칰mero de clases
presentes en el nodo:

-  **Para clasificaci칩n binaria (dos clases):** El valor m치ximo del
   칤ndice de Gini es 0,5. Esto ocurre cuando las dos clases est치n
   distribuidas equitativamente en el nodo, es decir, cuando
   :math:`洧녷_1=洧녷_2=0,5`

-  **Para m칰ltiples clases (m치s de dos clases):** El valor m치ximo del
   칤ndice de Gini es :math:`1-\frac{1}{n}`, :math:`n` donde es el n칰mero
   de clases. Esto ocurre cuando todas las clases est치n distribuidas
   equitativamente en el nodo, es decir, cuando :math:`洧녷_洧녰=\frac{1}{n}`
   para cada clase :math:`洧녰`.

**2. Entrop칤a:** Mide la cantidad de aleatoriedad o incertidumbre en los
datos. La entrop칤a se calcula como:

.. math::  H_i = -\sum_{k=1}^n{p_{i,k}log_2\left(p_{i,k}\right)}  

El objetivo es reducir la entrop칤a despu칠s de la divisi칩n, lo que se
lleva a nodos m치s puros.

**Poda (Pruning):**

Para evitar el sobreajuste, se utiliza la poda:

**Pre-Poda:** Detener el crecimiento del 치rbol antes de que se torne
complejo mediante la limitaci칩n de la profundidad m치xima o el n칰mero
m칤nimo de muestras en un nodo.

La pre-poda implica limitar el crecimiento del 치rbol durante su
construcci칩n para evitar que se torne demasiado complejo y sobreajuste
los datos de entrenamiento. En ``scikit-learn``, esto se puede lograr
ajustando ciertos hiperpar치metros al crear el modelo del 치rbol de
decisi칩n. A continuaci칩n, se presentan algunos de los hiperpar치metros
clave y c칩mo configurarlos:

-  ``max_depth``: Limita la profundidad m치xima del 치rbol.

-  ``min_samples_split``: N칰mero m칤nimo de muestras necesarias para
   dividir un nodo.

-  ``min_samples_leaf``: N칰mero m칤nimo de muestras que debe contener un
   nodo hoja.

-  ``max_leaf_nodes``: N칰mero m치ximo de nodos hoja.

Optimizaci칩n de Hiperpar치metros:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Para evitar el sobreajuste, se deben ajustar los hiperpar치metros del
modelo. Los m치s importantes son:

-  ``criterion``: Medida de calidad del clasificador (``gini`` o
   ``entropy``). Por defecto ``gini``.

-  ``splitter``: Estrategia para la divisi칩n en cada nodo (``best`` o
   ``random``). Por defecto ``best``.

-  ``max_depth``: Profundidad m치xima del 치rbol. Por defecto ``None``.

-  ``min_samples_split``: N칰mero m칤nimo de muestras requeridas para
   dividir un nodo. Por defecto ``2``.

-  ``min_samples_leaf``: N칰mero m칤nimo de muestras requeridas para estar
   en un nodo hoja. Por defecto ``1``.

-  ``max_leaf_nodes``: N칰mero m치ximo de nodos hoja. Por defecto
   ``None``.

Efecto de cambiar los hiperpar치metros:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

En los 치rboles de decisi칩n, ajustar los hiperpar치metros es crucial para
controlar la complejidad del modelo y prevenir el sobreajuste.

``max_depth``

Define la profundidad m치xima del 치rbol.

-  **Efecto de aumentar:** Permite que el 치rbol crezca m치s y capture m치s
   detalles de los datos de entrenamiento, lo que puede llevar a un
   sobreajuste (el 치rbol se ajusta demasiado a los datos de
   entrenamiento y puede no generalizar bien en datos nuevos).

-  **Efecto de disminuir:** Limita la complejidad del modelo, lo que
   puede ayudar a evitar el sobreajuste pero puede resultar en un modelo
   que no captura suficiente informaci칩n de los datos (subajuste).

``min_samples_split``

N칰mero m칤nimo de muestras requeridas para dividir un nodo.

-  **Efecto de aumentar:** Aumenta la cantidad de muestras necesarias
   para realizar una divisi칩n, lo que puede llevar a 치rboles m치s simples
   y menos profundos, ayudando a prevenir el sobreajuste.

-  **Efecto de disminuir:** Reduce el n칰mero de muestras necesarias para
   dividir un nodo, lo que puede llevar a 치rboles m치s complejos y
   profundos, aumentando el riesgo de sobreajuste.

``min_samples_leaf``

N칰mero m칤nimo de muestras que debe contener un nodo hoja.

-  **Efecto de aumentar:** Asegura que los nodos hoja contengan un mayor
   n칰mero de muestras, lo que puede suavizar el modelo y prevenir el
   sobreajuste, pero tambi칠n puede hacer que el modelo pierda detalles
   importantes.

-  **Efecto de disminuir:** Permite nodos hoja con menos muestras, lo
   que puede hacer el modelo m치s detallado pero tambi칠n m치s susceptible
   al sobreajuste.

``max_leaf_nodes``

N칰mero m치ximo de nodos hoja en el 치rbol.

-  **Efecto de aumentar:** Permite m치s nodos hoja, lo que puede hacer el
   modelo m치s complejo y preciso, pero tambi칠n aumenta el riesgo de
   sobreajuste.

-  **Efecto de disminuir:** Limita el n칰mero de nodos hoja,
   simplificando el modelo y ayudando a prevenir el sobreajuste, pero
   puede hacer que el modelo pierda detalles y subajuste.

``splitter``

Estrategia utilizada para dividir en cada nodo. Las opciones comunes son
``best`` y ``random``.

-  **Efecto de usar** ``best``\ **:** Selecciona la mejor divisi칩n
   posible basada en el criterio de impureza. Tiende a producir 치rboles
   m치s precisos, pero puede ser m치s lento.

-  **Efecto de usar** ``random``\ **:** Selecciona la mejor divisi칩n
   entre un subconjunto aleatorio de caracter칤sticas. Puede hacer el
   modelo m치s r치pido y a veces puede ayudar a prevenir el sobreajuste al
   introducir variabilidad.
