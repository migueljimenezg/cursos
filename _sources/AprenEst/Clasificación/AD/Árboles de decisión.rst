√Årboles de decisi√≥n
-------------------

Los √°rboles de decisi√≥n son un m√©todo de Machine Learning utilizado
tanto para clasificaci√≥n como para regresi√≥n. Estos algoritmos son
capaces de ajustar conjuntos de datos complejos y se caracterizan por su
capacidad de generar modelos f√°ciles de interpretar y visualizar.

**Conceptos clave:**

-  **Nodo ra√≠z:** El nodo superior del √°rbol que contiene toda la
   poblaci√≥n de datos. Desde aqu√≠ se inician las divisiones.

-  **Nodos internos:** Representan decisiones basadas en el valor de
   ciertos atributos.

-  **Hojas:** Representan la salida del modelo (clase en clasificaci√≥n o
   valor en regresi√≥n).

-  **Ramas:** Conectan nodos y representan el resultado de una prueba
   aplicada en un nodo.

**Construcci√≥n del √Årbol:**

La construcci√≥n de un √°rbol de decisi√≥n implica los siguientes pasos:

-  **Selecci√≥n de la caracter√≠stica de divisi√≥n:** Se selecciona la
   caracter√≠stica que mejor divide el conjunto de datos utilizando
   criterios como el √≠ndice de Gini, la entrop√≠a o la ganancia de
   informaci√≥n.

-  **Divisi√≥n del nodo:** Dividir el nodo actual en dos o m√°s
   subconjuntos basados en el valor de la caracter√≠stica seleccionada.

-  **Repetici√≥n:** Repetir el proceso de selecci√≥n de caracter√≠sticas y
   divisi√≥n hasta que se cumpla un criterio de parada, como alcanzar una
   profundidad m√°xima o un n√∫mero m√≠nimo de muestras en un nodo.

**Ventajas:**

-  **Interpretabilidad:** Son f√°ciles de entender y visualizar.

-  **Preparaci√≥n m√≠nima de datos:** No requieren escalado o centrado de
   caracter√≠sticas.

-  **Flexibilidad:** Pueden manejar tanto variables categ√≥ricas como
   num√©ricas.

**Desventajas:**

-  **Sobreajuste:** Tienden a sobreajustar los datos de entrenamiento si
   no se podan adecuadamente.

-  **Inestabilidad:** Peque√±as variaciones en los datos pueden producir
   √°rboles completamente diferentes.

-  **Sesgo en caracter√≠sticas categ√≥ricas con muchos niveles:** Pueden
   preferir caracter√≠sticas con muchos niveles, lo que puede llevar a
   divisiones menos significativas.

.. figure:: DecisionTree.png
   :alt: DecisionTree

   DecisionTree

Metodolog√≠a:
~~~~~~~~~~~~

Los √°rboles de decisi√≥n para clasificaci√≥n realizan la clasificaci√≥n en
dos o m√°s variables discretas. La metodolog√≠a de los √°rboles de
clasificaci√≥n se basa en dividir el espacio de las variables de entrada
con el objetivo de separarlo en regiones de elementos que pertenecen a
una clase particular. Este proceso es recursivo y el √°rbol crece a
medida que se realizan m√°s particiones, lo que se llama profundidad.

**Medidas de Impureza:**

**1. Impureza de Gini:** Mide la probabilidad de que un elemento
seleccionado al azar sea clasificado incorrectamente. La impureza de
Gini se calcula como:

.. math::  G_i = 1- \sum_{k=1}^n{p_{i,k}^2} 

Donde :math:`p_{i,k}` es la proporci√≥n de elementos de la clase k entre
los elementos en el nodo i.

-  Gini < 0,1: Excelente. El nodo es casi puro.

-  Gini entre 0,1 y 0,3: Bueno. El nodo tiene una pureza razonablemente
   alta.

-  Gini entre 0,3 y 0,4: Aceptable. El nodo tiene cierta impureza, pero
   a√∫n puede ser √∫til.

-  Gini > 0,4: Pobre. El nodo es bastante impuro y podr√≠a no ser muy
   √∫til para la clasificaci√≥n.

**Valor m√°ximo del √≠ndice de Gini:**

El valor m√°ximo del √≠ndice de Gini depende del n√∫mero de clases
presentes en el nodo:

-  **Para clasificaci√≥n binaria (dos clases):** El valor m√°ximo del
   √≠ndice de Gini es 0,5. Esto ocurre cuando las dos clases est√°n
   distribuidas equitativamente en el nodo, es decir, cuando
   :math:`ùëù_1=ùëù_2=0,5`

-  **Para m√∫ltiples clases (m√°s de dos clases):** El valor m√°ximo del
   √≠ndice de Gini es :math:`1-\frac{1}{n}`, :math:`n` donde es el n√∫mero
   de clases. Esto ocurre cuando todas las clases est√°n distribuidas
   equitativamente en el nodo, es decir, cuando :math:`ùëù_ùëñ=\frac{1}{n}`
   para cada clase :math:`ùëñ`.

**2. Entrop√≠a:** Mide la cantidad de aleatoriedad o incertidumbre en los
datos. La entrop√≠a se calcula como:

.. math::  H_i = -\sum_{k=1}^n{p_{i,k}log_2\left(p_{i,k}\right)}  

El objetivo es reducir la entrop√≠a despu√©s de la divisi√≥n, lo que se
lleva a nodos m√°s puros.

**Ejemplo con 5 manzanas üçé y 5 naranjas üçä**

Tenemos una caja con **10 frutas**:

-  5 manzanas

-  5 naranjas

Entonces, las probabilidades son:

:math:`p(\text{manzana}) = 0.5`

:math:`p(\text{naranja}) = 0.5`

--------------

**1. √çndice de Gini**

La f√≥rmula es:

.. math::


   Gini = 1 - p(\text{manzana})^2 - p(\text{naranja})^2

Sustituyendo:

.. math::


   Gini = 1 - 0.5^2 - 0.5^2 = 1 - 0.25 - 0.25 = 0.5

üëâ El Gini vale **0.5**, que es el valor m√°ximo posible cuando hay dos
clases (mezcla perfecta).

--------------

**2. Entrop√≠a**

La f√≥rmula es:

.. math::


   H = -\sum p_i \log_2(p_i)

Sustituyendo:

.. math::


   H = -(0.5 \log_2 0.5 + 0.5 \log_2 0.5)

.. math::


   H = -(0.5 \cdot -1 + 0.5 \cdot -1) = 1

üëâ La entrop√≠a vale **1 bit**, que tambi√©n representa la m√°xima
incertidumbre (mezcla perfecta).

--------------

**3. Nodo puro**

Si tuvi√©ramos solo manzanas (10 de 10):

-  :math:`p(\text{manzana}) = 1`

-  :math:`p(\text{naranja}) = 0`

Entonces:

.. math::


   Gini = 1 - 1^2 - 0^2 = 0

.. math::


   H = -(1 \cdot \log_2 1 + 0 \cdot \log_2 0) = 0

üëâ Tanto el Gini como la Entrop√≠a valen **0**, indicando que el grupo es
completamente puro (sin mezcla).

--------------

**Resumen intuitivo üéØ**

-  :math:`Gini` mide qu√© tan mezclado est√° el grupo:

   -  0 = nada mezclado (puro).

   -  0.5 = mezcla perfecta (50% ‚Äì 50%).

-  :math:`H` (entrop√≠a) mide el desorden o sorpresa:

   -  0 = sin sorpresa (puro).

   -  1 = m√°xima sorpresa (equilibrio perfecto).

**Poda (Pruning):**

Para evitar el sobreajuste, se utiliza la poda:

**Pre-Poda:** Detener el crecimiento del √°rbol antes de que se torne
complejo mediante la limitaci√≥n de la profundidad m√°xima o el n√∫mero
m√≠nimo de muestras en un nodo.

La pre-poda implica limitar el crecimiento del √°rbol durante su
construcci√≥n para evitar que se torne demasiado complejo y sobreajuste
los datos de entrenamiento. En ``scikit-learn``, esto se puede lograr
ajustando ciertos hiperpar√°metros al crear el modelo del √°rbol de
decisi√≥n. A continuaci√≥n, se presentan algunos de los hiperpar√°metros
clave y c√≥mo configurarlos:

-  ``max_depth``: Limita la profundidad m√°xima del √°rbol.

-  ``min_samples_split``: N√∫mero m√≠nimo de muestras necesarias para
   dividir un nodo.

-  ``min_samples_leaf``: N√∫mero m√≠nimo de muestras que debe contener un
   nodo hoja.

-  ``max_leaf_nodes``: N√∫mero m√°ximo de nodos hoja.

Optimizaci√≥n de Hiperpar√°metros:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Para evitar el sobreajuste, se deben ajustar los hiperpar√°metros del
modelo. Los m√°s importantes son:

-  ``criterion``: Medida de calidad del clasificador (``gini`` o
   ``entropy``). Por defecto ``gini``.

-  ``splitter``: Estrategia para la divisi√≥n en cada nodo (``best`` o
   ``random``). Por defecto ``best``.

-  ``max_depth``: Profundidad m√°xima del √°rbol. Por defecto ``None``.

-  ``min_samples_split``: N√∫mero m√≠nimo de muestras requeridas para
   dividir un nodo. Por defecto ``2``.

-  ``min_samples_leaf``: N√∫mero m√≠nimo de muestras requeridas para estar
   en un nodo hoja. Por defecto ``1``.

-  ``max_leaf_nodes``: N√∫mero m√°ximo de nodos hoja. Por defecto
   ``None``.

Efecto de cambiar los hiperpar√°metros:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

En los √°rboles de decisi√≥n, ajustar los hiperpar√°metros es crucial para
controlar la complejidad del modelo y prevenir el sobreajuste.

``max_depth``

Define la profundidad m√°xima del √°rbol.

-  **Efecto de aumentar:** Permite que el √°rbol crezca m√°s y capture m√°s
   detalles de los datos de entrenamiento, lo que puede llevar a un
   sobreajuste (el √°rbol se ajusta demasiado a los datos de
   entrenamiento y puede no generalizar bien en datos nuevos).

-  **Efecto de disminuir:** Limita la complejidad del modelo, lo que
   puede ayudar a evitar el sobreajuste pero puede resultar en un modelo
   que no captura suficiente informaci√≥n de los datos (subajuste).

``min_samples_split``

N√∫mero m√≠nimo de muestras requeridas para dividir un nodo.

-  **Efecto de aumentar:** Aumenta la cantidad de muestras necesarias
   para realizar una divisi√≥n, lo que puede llevar a √°rboles m√°s simples
   y menos profundos, ayudando a prevenir el sobreajuste.

-  **Efecto de disminuir:** Reduce el n√∫mero de muestras necesarias para
   dividir un nodo, lo que puede llevar a √°rboles m√°s complejos y
   profundos, aumentando el riesgo de sobreajuste.

``min_samples_leaf``

N√∫mero m√≠nimo de muestras que debe contener un nodo hoja.

-  **Efecto de aumentar:** Asegura que los nodos hoja contengan un mayor
   n√∫mero de muestras, lo que puede suavizar el modelo y prevenir el
   sobreajuste, pero tambi√©n puede hacer que el modelo pierda detalles
   importantes.

-  **Efecto de disminuir:** Permite nodos hoja con menos muestras, lo
   que puede hacer el modelo m√°s detallado pero tambi√©n m√°s susceptible
   al sobreajuste.

``max_leaf_nodes``

N√∫mero m√°ximo de nodos hoja en el √°rbol.

-  **Efecto de aumentar:** Permite m√°s nodos hoja, lo que puede hacer el
   modelo m√°s complejo y preciso, pero tambi√©n aumenta el riesgo de
   sobreajuste.

-  **Efecto de disminuir:** Limita el n√∫mero de nodos hoja,
   simplificando el modelo y ayudando a prevenir el sobreajuste, pero
   puede hacer que el modelo pierda detalles y subajuste.

``splitter``

Estrategia utilizada para dividir en cada nodo. Las opciones comunes son
``best`` y ``random``.

-  **Efecto de usar** ``best``\ **:** Selecciona la mejor divisi√≥n
   posible basada en el criterio de impureza. Tiende a producir √°rboles
   m√°s precisos, pero puede ser m√°s lento.

-  **Efecto de usar** ``random``\ **:** Selecciona la mejor divisi√≥n
   entre un subconjunto aleatorio de caracter√≠sticas. Puede hacer el
   modelo m√°s r√°pido y a veces puede ayudar a prevenir el sobreajuste al
   introducir variabilidad.
