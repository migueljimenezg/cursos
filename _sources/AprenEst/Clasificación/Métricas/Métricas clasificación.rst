M√©tricas para Evaluaci√≥n de Modelos de Clasificaci√≥n
----------------------------------------------------

Cuando entrenamos un modelo de clasificaci√≥n, no basta con ver si
‚Äúacierta‚Äù o ‚Äúfalla‚Äù.

Necesitamos m√©tricas que nos digan **qu√© tan bueno es el modelo y en qu√©
se equivoca**.

Estas m√©tricas se basan en una tabla muy importante: la **matriz de
confusi√≥n**.

--------------

Matriz de Confusi√≥n
~~~~~~~~~~~~~~~~~~~

La **matriz de confusi√≥n** compara lo que el modelo predijo con la
realidad.

Imagina que queremos clasificar si un cliente pagar√° un pr√©stamo (**s√≠ =
1**, **no = 0**).

================= ======================= =======================
\                 Predicho Positivo       Predicho Negativo
================= ======================= =======================
**Real Positivo** TP (Verdadero Positivo) FN (Falso Negativo)
**Real Negativo** FP (Falso Positivo)     TN (Verdadero Negativo)
================= ======================= =======================

-  **TP:** el cliente realmente pag√≥, y el modelo predijo que s√≠.

-  **TN:** el cliente no pag√≥, y el modelo predijo que no.

-  **FP:** el cliente no pag√≥, pero el modelo predijo que s√≠ (error
   grave para el banco).

-  **FN:** el cliente pag√≥, pero el modelo predijo que no.

**Ejemplo intuitivo:**

Un clasificador perfecto tendr√≠a solo TP y TN, es decir, solo valores en
la diagonal de la matriz.

--------------

-  :math:`TP` son los verdaderos positivos (True Positives)
   **(correcto)**. La clase es positiva (clase 1) y el clasificador la
   reconoce correctamente como tal (verdadero positivo).

-  :math:`TN` son los verdaderos negativos (True Negatives)
   **(correcto)**. La clase es negativa (clase 0) y el clasificador la
   reconoce correctamente como tal (verdadero negativo).

-  :math:`FP` son los falsos positivos (False Positives) **(mal
   clasificado)**. La clase es negativa, pero el clasificador la
   etiqueta como positiva (falso positivo). Es un 0 clasificado como 1.

-  :math:`FN` on los falsos negativos (False Negatives) **(mal
   clasificado)**. La clase es positiva, pero el clasificador la
   etiqueta como negativa (falso negativo). Es un 1 clasificado como 0.

Cada fila en una matriz de confusi√≥n representa una clase real, mientras
que cada columna representa una clase predicha.

.. figure:: MatrizConfusion.JPG
   :alt: MatrizConfusion

   MatrizConfusion

.. figure:: MatrizConfusion2.JPG
   :alt: MatrizConfusion2

   MatrizConfusion2

--------------

Accuracy (Exactitud)
~~~~~~~~~~~~~~~~~~~~

La **exactitud** es la m√©trica m√°s conocida: mide qu√© porcentaje de
veces acert√≥ el modelo.

.. math::


   Accuracy = \frac{TP + TN}{TP + TN + FP + FN}

-  F√°cil de interpretar.

-  Puede enga√±ar si hay clases **desbalanceadas**.

La **tasa de error de un clasificador (E)** es la frecuencia de errores
cometidos por el clasificador sobre un conjunto dado, es decir, la tasa
de error es 1 menos Accuracy.

.. math::  E = \frac{FP + FN}{TP+TN+FP + FN} = 1 - Accuracy 

**Ejemplo:**

Si el 95% de los clientes paga el pr√©stamo, un modelo que siempre
predice ‚Äús√≠ paga‚Äù tendr√° **95% de accuracy**, ¬°pero no sirve para
detectar morosos!

--------------

Precisi√≥n (Precision)
~~~~~~~~~~~~~~~~~~~~~

La **precisi√≥n** responde a:

üëâ ‚ÄúDe todos los clientes que el modelo predijo como **buenos
pagadores**, ¬øcu√°ntos realmente lo eran?‚Äù

.. math::


   Precision = \frac{TP}{TP + FP}

-  Evita dar ‚Äúfalsas alarmas‚Äù.

-  Importante cuando los **falsos positivos** son costosos.

**Ejemplo:**

En recomendaciones de productos en un e-commerce, preferimos que las
sugerencias sean pocas pero correctas.

--------------

La precisi√≥n es la proporci√≥n de verdaderos positivos entre el total de
predicciones positivas. Indica qu√© tan preciso es el modelo al predecir
la clase positiva. En otras palabras, la precisi√≥n mide la exactitud de
las predicciones positivas realizadas por el modelo.

La precisi√≥n se utiliza t√≠picamente junto con otra m√©trica llamada
recall, tambi√©n conocida como sensibilidad o tasa de verdaderos
positivos (TPR).

--------------

Recall o Sensibilidad
~~~~~~~~~~~~~~~~~~~~~

El **recall** responde a:

üëâ ‚ÄúDe todos los clientes que realmente **iban a pagar**, ¬øcu√°ntos fueron
detectados correctamente por el modelo?‚Äù

.. math::


   Recall = \frac{TP}{TP + FN}

-  Evita ‚Äúdejar pasar‚Äù casos importantes.

-  Importante cuando los **falsos negativos** son costosos.

**Ejemplo:**

En fraude financiero, es mejor atrapar a casi todos los fraudes (alto
recall), aunque algunas transacciones leg√≠timas sean marcadas como
sospechosas.

--------------

La tensi√≥n entre Precisi√≥n y Recall
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

-  **Bajar el umbral de decisi√≥n:** el modelo predice m√°s positivos ‚Üí
   sube el recall pero baja la precisi√≥n.

-  **Subir el umbral de decisi√≥n:** el modelo es m√°s estricto ‚Üí sube la
   precisi√≥n pero baja el recall.

Esto se llama **trade-off entre precisi√≥n y recall**.

--------------

La sensibilidad o recall es la proporci√≥n de verdaderos positivos entre
el total de positivos reales. Mide la capacidad del modelo para
identificar correctamente las instancias de la clase positiva.

A menudo es conveniente combinar la precisi√≥n y el recall en una sola
m√©trica llamada F score, especialmente si necesitas una forma sencilla
de comparar dos clasificadores. El F score es la media arm√≥nica de la
precisi√≥n y el recall.

**Ejemplos de aplicaci√≥n para alto recall**

-  **Detecci√≥n de Fraude:** Si est√°s desarrollando un sistema para
   detectar fraudes en transacciones financieras, podr√≠as preferir un
   alto recall. Es decir, quieres capturar todos los casos posibles de
   fraude, incluso si esto significa tener algunos falsos positivos
   (transacciones no fraudulentas clasificadas como fraudulentas).

-  **Diagn√≥stico M√©dico:** En un sistema de diagn√≥stico m√©dico para
   detectar una enfermedad grave, podr√≠as preferir un alto recall para
   asegurarte de que la mayor√≠a de los casos de la enfermedad sean
   detectados, incluso si hay algunos falsos positivos. De manera
   similar, un diagn√≥stico m√©dico incorrecto es a menudo m√°s costoso que
   no tener diagn√≥stico, pero un diagn√≥stico incorrecto puede resultar
   en elegir un tratamiento que haga m√°s da√±o que bien.

Por otro lado, en algunos casos la precisi√≥n es m√°s importante que el
recall. Por ejemplo, cuando compras algo en un sitio web, a menudo
aparece un mensaje como: ‚ÄúLos clientes que compraron X tambi√©n compraron
Y‚Äù. En este contexto, el valor del recall no es tan importante porque no
es crucial que el sistema identifique todos los art√≠culos que los
clientes podr√≠an querer. Lo fundamental es que los clientes est√©n
satisfechos con las recomendaciones que reciben. Si las recomendaciones
son precisas y relevantes, los clientes estar√°n m√°s inclinados a
considerar y aceptar estas sugerencias. De lo contrario, si las
recomendaciones son inexactas o irrelevantes, los clientes las ignorar√°n
en el futuro, disminuyendo la efectividad del sistema de
recomendaciones.

F1 Score
~~~~~~~~

El **F1 Score** combina precisi√≥n y recall en una sola m√©trica.

Es la **media arm√≥nica** de ambas:

.. math::


   F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}

-  √ötil cuando hay clases desbalanceadas.

-  Solo ser√° alto si **ambos** (precisi√≥n y recall) son altos.

| **Ejemplo:**
| En un diagn√≥stico m√©dico, no basta con detectar muchos enfermos
  (recall) si la mitad de los diagnosticados est√°n sanos (precisi√≥n
  baja). El F1 da una medida equilibrada.

--------------

La puntuaci√≥n F1 es la media arm√≥nica de la precisi√≥n y la sensibilidad.
Es √∫til cuando se necesita un equilibrio entre precisi√≥n y sensibilidad.
Es especialmente √∫til en contextos donde no solo es importante capturar
la mayor cantidad de instancias positivas posibles (recall), sino
tambi√©n asegurarse de que las predicciones positivas sean correctas
(precisi√≥n).

El clasificador solo obtendr√° una alta F score si tanto el recall como
la precisi√≥n son altos.

El F score favorece a los clasificadores que tienen precisi√≥n y recall
similares. Esto no siempre es lo que deseas: en algunos contextos te
importa m√°s la precisi√≥n, y en otros contextos realmente te importa el
recall. Por ejemplo, si entrenas un clasificador para detectar videos
que sean seguros para ni√±os, probablemente prefieras un clasificador que
rechace muchos buenos videos (bajo recall) pero mantenga solo los
seguros (alta precisi√≥n), en lugar de un clasificador que tenga un
recall mucho mayor pero permita que aparezcan algunos videos realmente
inapropiados en tu producto (en tales casos, incluso podr√≠as querer
agregar una revisi√≥n humana para verificar la selecci√≥n de videos del
clasificador) (G√©ron, 2019).

Por otro lado, supongamos que entrenas un clasificador para detectar
ladrones en im√°genes de vigilancia: probablemente est√© bien si tu
clasificador tiene solo un 30% de precisi√≥n siempre y cuando tenga un
99% de recall (claro, los guardias de seguridad recibir√°n algunas
alertas falsas, pero casi todos los ladrones ser√°n atrapados).

Desafortunadamente, no puedes tener ambos al mismo tiempo: aumentar la
precisi√≥n reduce el recall, y viceversa. Esto se llama la compensaci√≥n
entre precisi√≥n y recall.

Especificidad
~~~~~~~~~~~~~

La **especificidad** es lo opuesto al recall, pero aplicada a la clase
negativa.

üëâ ‚ÄúDe todos los clientes que realmente **no pagaron**, ¬øcu√°ntos fueron
clasificados correctamente como malos pagadores?‚Äù

.. math::


   Specificity = \frac{TN}{TN + FP}

-  Se usa mucho en medicina junto con el recall.

--------------

Curva Precisi√≥n / Recall
~~~~~~~~~~~~~~~~~~~~~~~~

Si movemos el **umbral de decisi√≥n** del modelo, la precisi√≥n y el
recall cambian.

-  **Umbral bajo:**

   -  Alta recall (detecta casi todos los positivos).

   -  Baja precisi√≥n (muchos falsos positivos).

-  **Umbral alto:**

   -  Alta precisi√≥n.

   -  Baja recall (se escapan muchos positivos).

**Visualizaci√≥n:** una curva que muestra la relaci√≥n entre precisi√≥n y
recall seg√∫n el umbral.

Muy √∫til cuando la clase positiva es rara (por ejemplo, fraude
bancario).

--------------

.. figure:: Metricas.png
   :alt: Metricas

   Metricas

.. figure:: Metricas_2.png
   :alt: Metricas_2

   Metricas_2

.. figure:: Metricas_2_1.png
   :alt: Metricas_2_1

   Metricas_2_1

Curva ROC y AUC-ROC
~~~~~~~~~~~~~~~~~~~

La **curva ROC** grafica:

-  **Eje Y:** Recall (TPR).

-  **Eje X:** Tasa de falsos positivos (FPR = 1 - Specificity).

El **AUC (√Årea Bajo la Curva ROC)** mide la capacidad global del modelo
para distinguir entre clases.

-  **AUC = 1:** modelo perfecto.

-  **AUC = 0.5:** modelo aleatorio.

**Ejemplo financiero:** comparar diferentes modelos de scoring
crediticio y elegir el que mejor separa buenos de malos pagadores.

--------------

La AUC-ROC mide la capacidad del modelo para distinguir entre clases,
representando el √°rea bajo la curva ROC. Esta curva grafica la tasa de
verdaderos positivos contra la tasa de falsos positivos en diferentes
umbrales de decisi√≥n. La FPR es la proporci√≥n de instancias negativas
clasificadas incorrectamente como positivas, y es igual a 1 menos la
especificidad (tasa de verdaderos negativos).

Una curva ROC ideal se aleja lo m√°s posible de la l√≠nea punteada de un
clasificador aleatorio, dirigi√©ndose hacia la esquina superior
izquierda. Un clasificador perfecto tiene una AUC-ROC de 1, mientras que
uno aleatorio tiene una AUC-ROC de 0.5.

La elecci√≥n entre la curva ROC y la curva precisi√≥n/recall depende del
contexto: se prefiere la curva precisi√≥n/recall cuando la clase positiva
es rara o cuando los falsos positivos son m√°s importantes que los falsos
negativos; en otros casos, se usa la curva ROC.

.. figure:: CurvaROC.JPG
   :alt: CurvaROC

   CurvaROC

Resumen:
~~~~~~~~

-  **Matriz de confusi√≥n:** la base de todas las m√©tricas.

-  **Accuracy:** bueno si las clases est√°n balanceadas.

-  **Precisi√≥n:** importante cuando FP son costosos (ej. dar un pr√©stamo
   a alguien riesgoso).

-  **Recall:** importante cuando FN son costosos (ej. no detectar un
   fraude).

-  **F1 Score:** balance entre precisi√≥n y recall.

-  **Especificidad:** detecci√≥n de la clase negativa.

-  **Curva PR:** √∫til en clases desbalanceadas.

-  **Curva ROC y AUC:** visi√≥n global de la capacidad del modelo.

Ejemplo: cuando una m√©trica ‚Äúalta‚Äù es enga√±osa (Fraude y Medicina)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

En problemas **desbalanceados** (pocos positivos), una m√©trica alta
puede dar una falsa sensaci√≥n de buen desempe√±o.

A continuaci√≥n, dos casos did√°cticos con **matrices de confusi√≥n**,
**c√°lculo de m√©tricas** y **conclusi√≥n pr√°ctica**.

--------------

Caso 1: Detecci√≥n de fraudes (clase positiva = fraude)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Contexto:** 10 000 transacciones, solo 100 son fraude (1%).

**Modelo A (ingenuo):** siempre predice ‚Äúno fraude‚Äù.

**Matriz de confusi√≥n (Modelo A):**

================= ================= =================
\                 Predicho Positivo Predicho Negativo
================= ================= =================
**Real Positivo** 0                 100
**Real Negativo** 0                 9 900
================= ================= =================

-  **Accuracy:** ((0 + 9 900) / 10 000 = 0.990) (**99%**)

-  **Precision:** indefinida (no predice positivos) ‚Üí por convenci√≥n
   **0**

-  **Recall (Sensibilidad):** (0 / (0 + 100) = 0) (**0%**)

**Conclusi√≥n:** la **accuracy es alt√≠sima** (99%), pero el modelo **no
detecta ning√∫n fraude** (recall = 0%).

**M√©trica que importa aqu√≠:** **Recall** (y tambi√©n **PR-AUC**) porque
perder fraudes (FN) es costoso.

--------------

**Modelo B (m√°s √∫til):** detecta 80 de 100 fraudes, pero comete 200
falsos positivos.

**Matriz de confusi√≥n (Modelo B):**

================= ================= =================
\                 Predicho Positivo Predicho Negativo
================= ================= =================
**Real Positivo** 80                20
**Real Negativo** 200               9 700
================= ================= =================

-  **Accuracy:** ((80 + 9 700) / 10 000 = 0.978) (**97.8%**) ‚Üê menor que
   antes

-  **Precision:** (80 / (80 + 200) = 0.286) (**28.6%**)

-  **Recall:** (80 / (80 + 20) = 0.80) (**80%**)

-  **F1:** (2¬∑(0.286¬∑0.80)/(0.286+0.80) ‚âà 0.421) (**42.1%**)

**Conclusi√≥n:** aunque la **accuracy baj√≥** (de 99% a 97.8%), el
**modelo B es mucho mejor** para el objetivo: captura el 80% de los
fraudes.

**Qu√© mirar:** en fraude, prioriza **Recall** (no perder fraudes) y
**F1** / **PR-AUC** sobre **Accuracy**.

--------------

Caso 2: Tamizaje en medicina (clase positiva = enfermedad)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Contexto:** 5 000 pacientes; 250 tienen la enfermedad (5%).

**Modelo C (muy ‚Äúconservador‚Äù):** solo marca positivos cuando est√° casi
seguro.

**Matriz de confusi√≥n (Modelo C):**

================= ================= =================
\                 Predicho Positivo Predicho Negativo
================= ================= =================
**Real Positivo** 100               150
**Real Negativo** 50                4 700
================= ================= =================

-  **Accuracy:** ((100 + 4 700) / 5 000 = 0.96) (**96%**)

-  **Precision:** (100 / (100 + 50) = 0.667) (**66.7%**) ‚Üê **alta**

-  **Recall:** (100 / (100 + 150) = 0.40) (**40%**) ‚Üê **baja**

**Conclusi√≥n:** la **precisi√≥n es alta**, pero el modelo **deja pasar
60%** de los casos enfermos (**FN** altos).

**Riesgo cl√≠nico:** pacientes no detectados pueden no recibir
tratamiento oportuno.

**Qu√© mirar:** en tamizaje, prioriza **Recall** (sensibilidad). Ajusta
el **umbral** para incrementar recall, aunque baje la precisi√≥n, y
comp√©nsalo con **segunda prueba** (confirmatoria) para filtrar falsos
positivos.

--------------

Lecciones clave (qu√© m√©trica usar seg√∫n el objetivo)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

-  **Fraude, seguridad, salud p√∫blica, fallas cr√≠ticas:** prioriza
   **Recall** (evitar FN). Complementa con **F1** y **PR-AUC**.

-  **Recomendadores, moderaci√≥n estricta, contenido infantil:** prioriza
   **Precision** (evitar FP). Ajusta **umbral** y considera **revisi√≥n
   humana**.

-  **Clases balanceadas y costo de error similar:** **Accuracy** puede
   ser √∫til, pero siempre verifica la **matriz de confusi√≥n**.

-  **Desbalance severo:** prefiere **PR-AUC** sobre **ROC-AUC** y
   reporta **Precision/Recall** a m√∫ltiples umbrales.

--------------
